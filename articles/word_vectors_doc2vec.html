<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Word vectors - doc2vec - text clustering • textTinyR</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Word vectors - doc2vec - text clustering">
<meta property="og:description" content="textTinyR">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">textTinyR</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.1.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/functionality_of_textTinyR_package.html">Functionality of the textTinyR package</a>
    </li>
    <li>
      <a href="../articles/word_vectors_doc2vec.html">Word vectors - doc2vec - text clustering</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlampros/textTinyR/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="word_vectors_doc2vec_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Word vectors - doc2vec - text clustering</h1>
                        <h4 class="author">Lampros Mouselimis</h4>
            
            <h4 class="date">2021-05-21</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/mlampros/textTinyR/blob/master/vignettes/word_vectors_doc2vec.Rmd"><code>vignettes/word_vectors_doc2vec.Rmd</code></a></small>
      <div class="hidden name"><code>word_vectors_doc2vec.Rmd</code></div>

    </div>

    
    
<p>This vignette discuss the new functionality, which is added in the textTinyR package (version 1.1.0). I’ll explain some of the functions by using the data and pre-processing steps of <a href="https://miguelmalvarez.com/2015/03/20/classifying-reuters-21578-collection-with-python-representing-the-data/">this blog-post</a>.</p>
<p><br></p>
<p>The following code chunks assume that the <em>nltk-corpus</em> is already downloaded and the <em>reticulate</em> package is installed,</p>
<p><br></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">NLTK</span> <span class="op">=</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu">import</span><span class="op">(</span><span class="st">"nltk.corpus"</span><span class="op">)</span>

<span class="va">text_reuters</span> <span class="op">=</span> <span class="va">NLTK</span><span class="op">$</span><span class="va">reuters</span> 


<span class="va">nltk</span> <span class="op">=</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu">import</span><span class="op">(</span><span class="st">"nltk"</span><span class="op">)</span>

<span class="co"># if the 'reuters' data is not already available then it can be downloaded from within R</span>

<span class="va">nltk</span><span class="op">$</span><span class="fu">download</span><span class="op">(</span><span class="st">'reuters'</span><span class="op">)</span>               </code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">documents</span> <span class="op">=</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">fileids</span><span class="op">(</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">documents</span><span class="op">)</span>


<span class="co"># List of categories</span>
<span class="va">categories</span> <span class="op">=</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">categories</span><span class="op">(</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">categories</span><span class="op">)</span>


<span class="co"># Documents in a category</span>
<span class="va">category_docs</span> <span class="op">=</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">fileids</span><span class="op">(</span><span class="st">"acq"</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">category_docs</span><span class="op">)</span>


<span class="va">one_doc</span> <span class="op">=</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">raw</span><span class="op">(</span><span class="st">"test/14843"</span><span class="op">)</span>

<span class="va">one_doc</span></code></pre></div>
<p><br></p>
<p>The collection originally consisted of 21,578 documents but a subset and split is traditionally used. The most common split is <em>Mod-Apte</em> which only considers categories that have at least one document in the training set and the test set. The <em>Mod-Apte</em> split has 90 categories with a training set of 7769 documents and a test set of 3019 documents.</p>
<p><br></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">documents</span> <span class="op">=</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">fileids</span><span class="op">(</span><span class="op">)</span>


<span class="co"># document ids for train - test</span>
<span class="va">train_docs_id</span> <span class="op">=</span> <span class="va">documents</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">documents</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/substr.html">substr</a></span><span class="op">(</span><span class="va">i</span>, <span class="fl">1</span>, <span class="fl">5</span><span class="op">)</span> <span class="op">==</span> <span class="st">"train"</span><span class="op">)</span><span class="op">)</span><span class="op">]</span>
<span class="va">test_docs_id</span> <span class="op">=</span> <span class="va">documents</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">documents</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/substr.html">substr</a></span><span class="op">(</span><span class="va">i</span>, <span class="fl">1</span>, <span class="fl">4</span><span class="op">)</span> <span class="op">==</span> <span class="st">"test"</span><span class="op">)</span><span class="op">)</span><span class="op">]</span>


<span class="va">train_docs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">train_docs_id</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">raw</span><span class="op">(</span><span class="va">train_docs_id</span><span class="op">[</span><span class="va">x</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>
<span class="va">test_docs</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">test_docs_id</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">raw</span><span class="op">(</span><span class="va">test_docs_id</span><span class="op">[</span><span class="va">x</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">train_docs</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">test_docs</span><span class="op">)</span>


<span class="co"># train - test labels  [ some categories might have more than one label (overlapping) ]</span>

<span class="va">train_labels</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">train_docs_id</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">categories</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>         
<span class="va">test_labels</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">test_docs_id</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">text_reuters</span><span class="op">$</span><span class="fu">categories</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>  </code></pre></div>
<p><br></p>
<div id="texttinyr---fasttextr---doc2vec---kmeans---cluster_medoids" class="section level3">
<h3 class="hasAnchor">
<a href="#texttinyr---fasttextr---doc2vec---kmeans---cluster_medoids" class="anchor"></a>textTinyR - fastTextR - doc2vec - kmeans - cluster_medoids</h3>
<p><br></p>
<p>First, I’ll perform the following pre-processing steps :</p>
<ul>
<li>convert to lower case</li>
<li>trim tokens</li>
<li>remove stopwords</li>
<li>porter stemming</li>
<li>keep words with minimum number of characters equal to 3</li>
</ul>
<p><br></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">concat</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">train_docs</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">test_docs</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">concat</span><span class="op">)</span>


<span class="va">clust_vec</span> <span class="op">=</span> <span class="fu">textTinyR</span><span class="fu">::</span><span class="fu"><a href="../reference/tokenize_transform_vec_docs.html">tokenize_transform_vec_docs</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">concat</span>, as_token <span class="op">=</span> <span class="cn">T</span>,
                                                   to_lower <span class="op">=</span> <span class="cn">T</span>, 
                                                   remove_punctuation_vector <span class="op">=</span> <span class="cn">F</span>,
                                                   remove_numbers <span class="op">=</span> <span class="cn">F</span>, 
                                                   trim_token <span class="op">=</span> <span class="cn">T</span>,
                                                   split_string <span class="op">=</span> <span class="cn">T</span>,
                                                   split_separator <span class="op">=</span> <span class="st">" \r\n\t.,;:()?!//"</span>, 
                                                   remove_stopwords <span class="op">=</span> <span class="cn">T</span>,
                                                   language <span class="op">=</span> <span class="st">"english"</span>, 
                                                   min_num_char <span class="op">=</span> <span class="fl">3</span>, 
                                                   max_num_char <span class="op">=</span> <span class="fl">100</span>,
                                                   stemmer <span class="op">=</span> <span class="st">"porter2_stemmer"</span>, 
                                                   threads <span class="op">=</span> <span class="fl">4</span>,
                                                   verbose <span class="op">=</span> <span class="cn">T</span><span class="op">)</span>

<span class="va">unq</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html">unique</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">clust_vec</span><span class="op">$</span><span class="va">token</span>, recursive <span class="op">=</span> <span class="cn">F</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">unq</span><span class="op">)</span>


<span class="co"># I'll build also the term matrix as I'll need the global-term-weights</span>

<span class="va">utl</span> <span class="op">=</span> <span class="fu">textTinyR</span><span class="fu">::</span><span class="va"><a href="../reference/sparse_term_matrix.html">sparse_term_matrix</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>vector_data <span class="op">=</span> <span class="va">concat</span>, file_data <span class="op">=</span> <span class="cn">NULL</span>,
                                        document_term_matrix <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">tm</span> <span class="op">=</span> <span class="va">utl</span><span class="op">$</span><span class="fu">Term_Matrix</span><span class="op">(</span>sort_terms <span class="op">=</span> <span class="cn">FALSE</span>, to_lower <span class="op">=</span> <span class="cn">T</span>, remove_punctuation_vector <span class="op">=</span> <span class="cn">F</span>,
                     remove_numbers <span class="op">=</span> <span class="cn">F</span>, trim_token <span class="op">=</span> <span class="cn">T</span>, split_string <span class="op">=</span> <span class="cn">T</span>, 
                     stemmer <span class="op">=</span> <span class="st">"porter2_stemmer"</span>,
                     split_separator <span class="op">=</span> <span class="st">" \r\n\t.,;:()?!//"</span>, remove_stopwords <span class="op">=</span> <span class="cn">T</span>,
                     language <span class="op">=</span> <span class="st">"english"</span>, min_num_char <span class="op">=</span> <span class="fl">3</span>, max_num_char <span class="op">=</span> <span class="fl">100</span>,
                     print_every_rows <span class="op">=</span> <span class="fl">100000</span>, normalize <span class="op">=</span> <span class="cn">NULL</span>, tf_idf <span class="op">=</span> <span class="cn">F</span>, 
                     threads <span class="op">=</span> <span class="fl">6</span>, verbose <span class="op">=</span> <span class="cn">T</span><span class="op">)</span>

<span class="va">gl_term_w</span> <span class="op">=</span> <span class="va">utl</span><span class="op">$</span><span class="fu">global_term_weights</span><span class="op">(</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">gl_term_w</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>For simplicity, I’ll use the <em>Reuters</em> data as input to the <em>fastTextR::skipgram_cbow</em> function. The data has to be first pre-processed and then saved to a file,</p>
<p><br></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"> <span class="va">save_dat</span> <span class="op">=</span> <span class="fu">textTinyR</span><span class="fu">::</span><span class="fu"><a href="../reference/tokenize_transform_vec_docs.html">tokenize_transform_vec_docs</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">concat</span>, as_token <span class="op">=</span> <span class="cn">T</span>, 
                                                   to_lower <span class="op">=</span> <span class="cn">T</span>, 
                                                   remove_punctuation_vector <span class="op">=</span> <span class="cn">F</span>,
                                                   remove_numbers <span class="op">=</span> <span class="cn">F</span>, trim_token <span class="op">=</span> <span class="cn">T</span>, 
                                                   split_string <span class="op">=</span> <span class="cn">T</span>, 
                                                   split_separator <span class="op">=</span> <span class="st">" \r\n\t.,;:()?!//"</span>,
                                                   remove_stopwords <span class="op">=</span> <span class="cn">T</span>, language <span class="op">=</span> <span class="st">"english"</span>, 
                                                   min_num_char <span class="op">=</span> <span class="fl">3</span>, max_num_char <span class="op">=</span> <span class="fl">100</span>, 
                                                   stemmer <span class="op">=</span> <span class="st">"porter2_stemmer"</span>, 
                                                   path_2folder <span class="op">=</span> <span class="st">"/path_to_your_folder/"</span>,
                                                   threads <span class="op">=</span> <span class="fl">1</span>,                     <span class="co"># whenever I save data to file set the number threads to 1</span>
                                                   verbose <span class="op">=</span> <span class="cn">T</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p><strong>UPDATE 11-04-2019</strong>: There is an <a href="https://github.com/mlampros/fastText">updated version of the fastText R package</a> which includes all the features of the ported <a href="https://github.com/facebookresearch/fastText">fasttext library</a>. Therefore the old <strong>fastTextR</strong> repository <strong>is archived</strong>. See also the corresponding <a href="http://mlampros.github.io/2019/04/11/fastText_updated_version/">blog-post</a>.</p>
<p><br></p>
<p>Then, I’ll load the previously saved data and I’ll use <a href="https://github.com/mlampros/fastTextR">fastTextR</a> to build the word-vectors,</p>
<p><br></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">PATH_INPUT</span> <span class="op">=</span> <span class="st">"/path_to_your_folder/output_token_single_file.txt"</span>

<span class="va">PATH_OUT</span> <span class="op">=</span> <span class="st">"/path_to_your_folder/rt_fst_model"</span>


<span class="va">vecs</span> <span class="op">=</span> <span class="fu">fastTextR</span><span class="fu">::</span><span class="fu">skipgram_cbow</span><span class="op">(</span>input_path <span class="op">=</span> <span class="va">PATH_INPUT</span>, output_path <span class="op">=</span> <span class="va">PATH_OUT</span>, 
                                method <span class="op">=</span> <span class="st">"skipgram"</span>, lr <span class="op">=</span> <span class="fl">0.075</span>, lrUpdateRate <span class="op">=</span> <span class="fl">100</span>, 
                                dim <span class="op">=</span> <span class="fl">300</span>, ws <span class="op">=</span> <span class="fl">5</span>, epoch <span class="op">=</span> <span class="fl">5</span>, minCount <span class="op">=</span> <span class="fl">1</span>, neg <span class="op">=</span> <span class="fl">5</span>, 
                                wordNgrams <span class="op">=</span> <span class="fl">2</span>, loss <span class="op">=</span> <span class="st">"ns"</span>, bucket <span class="op">=</span> <span class="fl">2e+06</span>,
                                minn <span class="op">=</span> <span class="fl">0</span>, maxn <span class="op">=</span> <span class="fl">0</span>, thread <span class="op">=</span> <span class="fl">6</span>, t <span class="op">=</span> <span class="fl">1e-04</span>, verbose <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>Before using one of the three methods, it would be better to reduce the initial dimensions of the word-vectors (rows of the matrix). So, I’ll keep the word-vectors for which the terms appear in the <em>Reuters</em> data set - <em>clust_vec$token</em> ( although it’s not applicable in this case, if the resulted word-vectors were based on external data - say the Wikipedia data - then their dimensions would be way larger and many of the terms would be redundant for the <em>Reuters</em> data set increasing that way the computation time considerably when invoking one of the doc2vec methods),</p>
<p><br></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>init =<span class="st"> </span>textTinyR<span class="op">::</span>Doc2Vec<span class="op">$</span><span class="kw">new</span>(<span class="dt">token_list =</span> clust_vec<span class="op">$</span>token, </span>
<span id="cb7-2"><a href="#cb7-2"></a>                              </span>
<span id="cb7-3"><a href="#cb7-3"></a>                              <span class="dt">word_vector_FILE =</span> <span class="st">"path_to_your_folder/rt_fst_model.vec"</span>,</span>
<span id="cb7-4"><a href="#cb7-4"></a>                              </span>
<span id="cb7-5"><a href="#cb7-5"></a>                              <span class="dt">print_every_rows =</span> <span class="dv">5000</span>, </span>
<span id="cb7-6"><a href="#cb7-6"></a>                              </span>
<span id="cb7-7"><a href="#cb7-7"></a>                              <span class="dt">verbose =</span> <span class="ot">TRUE</span>, </span>
<span id="cb7-8"><a href="#cb7-8"></a>                              </span>
<span id="cb7-9"><a href="#cb7-9"></a>                              <span class="dt">copy_data =</span> <span class="ot">FALSE</span>)                  <span class="co"># use of external pointer</span></span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a></span>
<span id="cb7-12"><a href="#cb7-12"></a>pre<span class="op">-</span>processing of input data starts ...</span>
<span id="cb7-13"><a href="#cb7-13"></a>File is successfully opened</span>
<span id="cb7-14"><a href="#cb7-14"></a>total.number.lines.processed.input<span class="op">:</span><span class="st"> </span><span class="dv">25000</span></span>
<span id="cb7-15"><a href="#cb7-15"></a>creation of index starts ...</span>
<span id="cb7-16"><a href="#cb7-16"></a>intersection of tokens and wordvec character strings starts ...</span>
<span id="cb7-17"><a href="#cb7-17"></a>modification of indices starts ...</span>
<span id="cb7-18"><a href="#cb7-18"></a>final processing of data starts ...</span>
<span id="cb7-19"><a href="#cb7-19"></a>File is successfully opened</span>
<span id="cb7-20"><a href="#cb7-20"></a>total.number.lines.processed.output<span class="op">:</span><span class="st"> </span><span class="dv">25000</span></span></code></pre></div>
<p><br></p>
<p>In case that <em>copy_data = TRUE</em> then the pre-processed data can be observed before invoking one of the ‘doc2vec’ methods,</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># res_wv = init$pre_processed_wv()                           </span>
<span class="co"># </span>
<span class="co"># str(res_wv)</span></code></pre></div>
<p><br></p>
<p>Then, I can use one of the three methods (<em>sum_sqrt</em>, <em>min_max_norm</em>, <em>idf</em>) to receive the transformed vectors. These methods are based on the following <em>blog-posts</em> (see especially <strong><span></span>www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur</strong> and <strong><span></span><a href="https://erogol.com/duplicate-question-detection-deep-learning/" class="uri">https://erogol.com/duplicate-question-detection-deep-learning/</a></strong> ),</p>
<p><br></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>doc2_sum =<span class="st"> </span>init<span class="op">$</span><span class="kw">doc2vec_methods</span>(<span class="dt">method =</span> <span class="st">"sum_sqrt"</span>, <span class="dt">threads =</span> <span class="dv">6</span>)</span>
<span id="cb9-2"><a href="#cb9-2"></a>doc2_norm =<span class="st"> </span>init<span class="op">$</span><span class="kw">doc2vec_methods</span>(<span class="dt">method =</span> <span class="st">"min_max_norm"</span>, <span class="dt">threads =</span> <span class="dv">6</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a>doc2_idf =<span class="st"> </span>init<span class="op">$</span><span class="kw">doc2vec_methods</span>(<span class="dt">method =</span> <span class="st">"idf"</span>, <span class="dt">global_term_weights =</span> gl_term_w, <span class="dt">threads =</span> <span class="dv">6</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>rows_cols =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">5</span></span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a>doc2_sum[rows_cols, rows_cols]</span>
<span id="cb9-8"><a href="#cb9-8"></a>doc2_norm[rows_cols, rows_cols]</span>
<span id="cb9-9"><a href="#cb9-9"></a>doc2_idf[rows_cols, rows_cols]</span>
<span id="cb9-10"><a href="#cb9-10"></a></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">dim</span>(doc2_sum)</span>
<span id="cb9-12"><a href="#cb9-12"></a>[<span class="dv">1</span>] <span class="dv">10788</span>   <span class="dv">300</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">dim</span>(doc2_norm)</span>
<span id="cb9-14"><a href="#cb9-14"></a>[<span class="dv">1</span>] <span class="dv">10788</span>   <span class="dv">300</span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">dim</span>(doc2_idf)</span>
<span id="cb9-16"><a href="#cb9-16"></a>[<span class="dv">1</span>] <span class="dv">10788</span>   <span class="dv">300</span></span></code></pre></div>
<p><br></p>
<p>For illustration, I’ll use the resulted word-vectors of the <em>sum_sqrt</em> method. The approach described can be used as an alternative to <em>Latent semantic indexing (LSI)</em> or <em>topic-modeling</em> in order to discover categories in text data (documents).</p>
<p><br></p>
<p>First, someone can seach for the optimal number of clusters using the <em>Optimal_Clusters_KMeans</em> function of the <em>ClusterR</em> package,</p>
<p><br></p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">scal_dat</span> <span class="op">=</span> <span class="fu">ClusterR</span><span class="fu">::</span><span class="fu">center_scale</span><span class="op">(</span><span class="va">doc2_sum</span><span class="op">)</span>     <span class="co"># center and scale the data</span>


<span class="va">opt_cl</span> <span class="op">=</span> <span class="fu">ClusterR</span><span class="fu">::</span><span class="fu">Optimal_Clusters_KMeans</span><span class="op">(</span><span class="va">scal_dat</span>, max_clusters <span class="op">=</span> <span class="fl">15</span>, 
                                           criterion <span class="op">=</span> <span class="st">"distortion_fK"</span>,
                                           fK_threshold <span class="op">=</span> <span class="fl">0.85</span>, num_init <span class="op">=</span> <span class="fl">3</span>, 
                                           max_iters <span class="op">=</span> <span class="fl">50</span>,
                                           initializer <span class="op">=</span> <span class="st">"kmeans++"</span>, tol <span class="op">=</span> <span class="fl">1e-04</span>, 
                                           plot_clusters <span class="op">=</span> <span class="cn">TRUE</span>,
                                           verbose <span class="op">=</span> <span class="cn">T</span>, tol_optimal_init <span class="op">=</span> <span class="fl">0.3</span>, 
                                           seed <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>Based on the output of the <em>Optimal_Clusters_KMeans</em> function, I’ll pick 5 as the optimal number of clusters in order to perform <em>k-means clustering</em>,</p>
<p><br></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>num_clust =<span class="st"> </span><span class="dv">5</span></span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>km =<span class="st"> </span>ClusterR<span class="op">::</span><span class="kw">KMeans_rcpp</span>(scal_dat, <span class="dt">clusters =</span> num_clust, <span class="dt">num_init =</span> <span class="dv">3</span>, <span class="dt">max_iters =</span> <span class="dv">50</span>,</span>
<span id="cb11-4"><a href="#cb11-4"></a>                           <span class="dt">initializer =</span> <span class="st">"kmeans++"</span>, <span class="dt">fuzzy =</span> T, <span class="dt">verbose =</span> F,</span>
<span id="cb11-5"><a href="#cb11-5"></a>                           <span class="dt">CENTROIDS =</span> <span class="ot">NULL</span>, <span class="dt">tol =</span> <span class="fl">1e-04</span>, <span class="dt">tol_optimal_init =</span> <span class="fl">0.3</span>, <span class="dt">seed =</span> <span class="dv">2</span>)</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="kw">table</span>(km<span class="op">$</span>clusters)</span>
<span id="cb11-9"><a href="#cb11-9"></a></span>
<span id="cb11-10"><a href="#cb11-10"></a>   <span class="dv">1</span>    <span class="dv">2</span>    <span class="dv">3</span>    <span class="dv">4</span>    <span class="dv">5</span> </span>
<span id="cb11-11"><a href="#cb11-11"></a> <span class="dv">713</span> <span class="dv">2439</span> <span class="dv">2393</span> <span class="dv">2607</span> <span class="dv">2636</span> </span></code></pre></div>
<p><br></p>
<p>As a follow up, someone can also perform <em>cluster-medoids</em> clustering using the <em>pearson-correlation</em> metric, which resembles the <em>cosine</em> distance ( the latter is frequently used for text clustering ),</p>
<p><br></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>kmed =<span class="st"> </span>ClusterR<span class="op">::</span><span class="kw">Cluster_Medoids</span>(scal_dat, <span class="dt">clusters =</span> num_clust, </span>
<span id="cb12-2"><a href="#cb12-2"></a>                                 <span class="dt">distance_metric =</span> <span class="st">"pearson_correlation"</span>,</span>
<span id="cb12-3"><a href="#cb12-3"></a>                                 <span class="dt">minkowski_p =</span> <span class="dv">1</span>, <span class="dt">threads =</span> <span class="dv">6</span>, <span class="dt">swap_phase =</span> <span class="ot">TRUE</span>, </span>
<span id="cb12-4"><a href="#cb12-4"></a>                                 <span class="dt">fuzzy =</span> <span class="ot">FALSE</span>, <span class="dt">verbose =</span> F, <span class="dt">seed =</span> <span class="dv">1</span>)</span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="kw">table</span>(kmed<span class="op">$</span>clusters)</span>
<span id="cb12-8"><a href="#cb12-8"></a></span>
<span id="cb12-9"><a href="#cb12-9"></a>   <span class="dv">1</span>    <span class="dv">2</span>    <span class="dv">3</span>    <span class="dv">4</span>    <span class="dv">5</span> </span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="dv">2396</span> <span class="dv">2293</span> <span class="dv">2680</span>  <span class="dv">875</span> <span class="dv">2544</span> </span></code></pre></div>
<p><br></p>
<p>Finally, the word-frequencies of the documents can be obtained using the <em>cluster_frequency</em> function, which groups the tokens (words) of the documents based on which cluster each document appears,</p>
<p><br></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>freq_clust =<span class="st"> </span>textTinyR<span class="op">::</span><span class="kw">cluster_frequency</span>(<span class="dt">tokenized_list_text =</span> clust_vec<span class="op">$</span>token, </span>
<span id="cb13-2"><a href="#cb13-2"></a>                                          <span class="dt">cluster_vector =</span> km<span class="op">$</span>clusters, <span class="dt">verbose =</span> T)</span>
<span id="cb13-3"><a href="#cb13-3"></a></span>
<span id="cb13-4"><a href="#cb13-4"></a>Time difference of <span class="fl">0.1762383</span> secs</span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="op">&gt;</span><span class="st"> </span>freq_clust</span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="op">$</span><span class="st">`</span><span class="dt">3</span><span class="st">`</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>         WORDS COUNTS</span>
<span id="cb14-5"><a href="#cb14-5"></a>   <span class="dv">1</span><span class="op">:</span><span class="st">      </span>mln   <span class="dv">8701</span></span>
<span id="cb14-6"><a href="#cb14-6"></a>   <span class="dv">2</span><span class="op">:</span><span class="st">      </span><span class="dv">000</span>   <span class="dv">6741</span></span>
<span id="cb14-7"><a href="#cb14-7"></a>   <span class="dv">3</span><span class="op">:</span><span class="st">      </span>cts   <span class="dv">6260</span></span>
<span id="cb14-8"><a href="#cb14-8"></a>   <span class="dv">4</span><span class="op">:</span><span class="st">      </span>net   <span class="dv">5949</span></span>
<span id="cb14-9"><a href="#cb14-9"></a>   <span class="dv">5</span><span class="op">:</span><span class="st">     </span>loss   <span class="dv">4628</span></span>
<span id="cb14-10"><a href="#cb14-10"></a>  <span class="op">---</span><span class="st">                </span></span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="dv">6417</span><span class="op">:</span><span class="st">    </span>vira<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-12"><a href="#cb14-12"></a><span class="dv">6418</span><span class="op">:</span><span class="st">    </span>gain<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="dv">6419</span><span class="op">:</span><span class="st">     </span>pwj<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="dv">6420</span><span class="op">:</span><span class="st"> </span>drummond      <span class="dv">1</span></span>
<span id="cb14-15"><a href="#cb14-15"></a><span class="dv">6421</span><span class="op">:</span><span class="st"> </span>parisian      <span class="dv">1</span></span>
<span id="cb14-16"><a href="#cb14-16"></a></span>
<span id="cb14-17"><a href="#cb14-17"></a><span class="op">$</span><span class="st">`</span><span class="dt">1</span><span class="st">`</span></span>
<span id="cb14-18"><a href="#cb14-18"></a>         WORDS COUNTS</span>
<span id="cb14-19"><a href="#cb14-19"></a>   <span class="dv">1</span><span class="op">:</span><span class="st">      </span>cts   <span class="dv">1303</span></span>
<span id="cb14-20"><a href="#cb14-20"></a>   <span class="dv">2</span><span class="op">:</span><span class="st">   </span>record    <span class="dv">696</span></span>
<span id="cb14-21"><a href="#cb14-21"></a>   <span class="dv">3</span><span class="op">:</span><span class="st">    </span>april    <span class="dv">669</span></span>
<span id="cb14-22"><a href="#cb14-22"></a>   <span class="dv">4</span><span class="op">:</span><span class="st">      </span><span class="er">&amp;</span>lt    <span class="dv">652</span></span>
<span id="cb14-23"><a href="#cb14-23"></a>   <span class="dv">5</span><span class="op">:</span><span class="st"> </span>dividend    <span class="dv">554</span></span>
<span id="cb14-24"><a href="#cb14-24"></a>  <span class="op">---</span><span class="st">                </span></span>
<span id="cb14-25"><a href="#cb14-25"></a><span class="dv">1833</span><span class="op">:</span><span class="st">     </span>hvt<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-26"><a href="#cb14-26"></a><span class="dv">1834</span><span class="op">:</span><span class="st">    </span>bang<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-27"><a href="#cb14-27"></a><span class="dv">1835</span><span class="op">:</span><span class="st">   </span>replac      <span class="dv">1</span></span>
<span id="cb14-28"><a href="#cb14-28"></a><span class="dv">1836</span><span class="op">:</span><span class="st">    </span>stbk<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-29"><a href="#cb14-29"></a><span class="dv">1837</span><span class="op">:</span><span class="st">     </span>bic<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-30"><a href="#cb14-30"></a></span>
<span id="cb14-31"><a href="#cb14-31"></a><span class="op">$</span><span class="st">`</span><span class="dt">4</span><span class="st">`</span></span>
<span id="cb14-32"><a href="#cb14-32"></a>         WORDS COUNTS</span>
<span id="cb14-33"><a href="#cb14-33"></a>    <span class="dv">1</span><span class="op">:</span><span class="st">     </span>mln   <span class="dv">6137</span></span>
<span id="cb14-34"><a href="#cb14-34"></a>    <span class="dv">2</span><span class="op">:</span><span class="st">     </span>pct   <span class="dv">5084</span></span>
<span id="cb14-35"><a href="#cb14-35"></a>    <span class="dv">3</span><span class="op">:</span><span class="st">    </span>dlrs   <span class="dv">4024</span></span>
<span id="cb14-36"><a href="#cb14-36"></a>    <span class="dv">4</span><span class="op">:</span><span class="st">    </span>year   <span class="dv">3397</span></span>
<span id="cb14-37"><a href="#cb14-37"></a>    <span class="dv">5</span><span class="op">:</span><span class="st"> </span>billion   <span class="dv">3390</span></span>
<span id="cb14-38"><a href="#cb14-38"></a>   <span class="op">---</span><span class="st">               </span></span>
<span id="cb14-39"><a href="#cb14-39"></a><span class="dv">10968</span><span class="op">:</span><span class="st">   </span>heijn      <span class="dv">1</span></span>
<span id="cb14-40"><a href="#cb14-40"></a><span class="dv">10969</span><span class="op">:</span><span class="st"> "behind      1</span></span>
<span id="cb14-41"><a href="#cb14-41"></a><span class="st">10970:    myo&gt;      1</span></span>
<span id="cb14-42"><a href="#cb14-42"></a><span class="st">10971:  "</span>favor      <span class="dv">1</span></span>
<span id="cb14-43"><a href="#cb14-43"></a><span class="dv">10972</span><span class="op">:</span><span class="st"> </span>wonder<span class="op">&gt;</span><span class="st">      </span><span class="dv">1</span></span>
<span id="cb14-44"><a href="#cb14-44"></a></span>
<span id="cb14-45"><a href="#cb14-45"></a><span class="op">$</span><span class="st">`</span><span class="dt">5</span><span class="st">`</span></span>
<span id="cb14-46"><a href="#cb14-46"></a>                  WORDS COUNTS</span>
<span id="cb14-47"><a href="#cb14-47"></a>    <span class="dv">1</span><span class="op">:</span><span class="st">              </span><span class="er">&amp;</span>lt   <span class="dv">4244</span></span>
<span id="cb14-48"><a href="#cb14-48"></a>    <span class="dv">2</span><span class="op">:</span><span class="st">            </span>share   <span class="dv">3748</span></span>
<span id="cb14-49"><a href="#cb14-49"></a>    <span class="dv">3</span><span class="op">:</span><span class="st">             </span>dlrs   <span class="dv">3274</span></span>
<span id="cb14-50"><a href="#cb14-50"></a>    <span class="dv">4</span><span class="op">:</span><span class="st">          </span>compani   <span class="dv">3184</span></span>
<span id="cb14-51"><a href="#cb14-51"></a>    <span class="dv">5</span><span class="op">:</span><span class="st">              </span>mln   <span class="dv">2659</span></span>
<span id="cb14-52"><a href="#cb14-52"></a>   <span class="op">---</span><span class="st">                        </span></span>
<span id="cb14-53"><a href="#cb14-53"></a><span class="dv">13059</span><span class="op">:</span><span class="st">        </span>often<span class="op">-</span>fat      <span class="dv">1</span></span>
<span id="cb14-54"><a href="#cb14-54"></a><span class="dv">13060</span><span class="op">:</span><span class="st"> </span>computerknowledg      <span class="dv">1</span></span>
<span id="cb14-55"><a href="#cb14-55"></a><span class="dv">13061</span><span class="op">:</span><span class="st">       </span>fibrinolyt      <span class="dv">1</span></span>
<span id="cb14-56"><a href="#cb14-56"></a><span class="dv">13062</span><span class="op">:</span><span class="st">           </span>hercul      <span class="dv">1</span></span>
<span id="cb14-57"><a href="#cb14-57"></a><span class="dv">13063</span><span class="op">:</span><span class="st">           </span>ceroni      <span class="dv">1</span></span>
<span id="cb14-58"><a href="#cb14-58"></a></span>
<span id="cb14-59"><a href="#cb14-59"></a><span class="op">$</span><span class="st">`</span><span class="dt">2</span><span class="st">`</span></span>
<span id="cb14-60"><a href="#cb14-60"></a>             WORDS COUNTS</span>
<span id="cb14-61"><a href="#cb14-61"></a>    <span class="dv">1</span><span class="op">:</span><span class="st">       </span>trade   <span class="dv">3077</span></span>
<span id="cb14-62"><a href="#cb14-62"></a>    <span class="dv">2</span><span class="op">:</span><span class="st">        </span>bank   <span class="dv">2578</span></span>
<span id="cb14-63"><a href="#cb14-63"></a>    <span class="dv">3</span><span class="op">:</span><span class="st">      </span>market   <span class="dv">2535</span></span>
<span id="cb14-64"><a href="#cb14-64"></a>    <span class="dv">4</span><span class="op">:</span><span class="st">         </span>pct   <span class="dv">2416</span></span>
<span id="cb14-65"><a href="#cb14-65"></a>    <span class="dv">5</span><span class="op">:</span><span class="st">        </span>rate   <span class="dv">2308</span></span>
<span id="cb14-66"><a href="#cb14-66"></a>   <span class="op">---</span><span class="st">                   </span></span>
<span id="cb14-67"><a href="#cb14-67"></a><span class="dv">13702</span><span class="op">:</span><span class="st">        "mfn      1</span></span>
<span id="cb14-68"><a href="#cb14-68"></a><span class="st">13703:         uk&gt;      1</span></span>
<span id="cb14-69"><a href="#cb14-69"></a><span class="st">13704:    honolulu      1</span></span>
<span id="cb14-70"><a href="#cb14-70"></a><span class="st">13705:        arap      1</span></span>
<span id="cb14-71"><a href="#cb14-71"></a><span class="st">13706: infinitesim      1</span></span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>freq_clust_kmed =<span class="st"> </span>textTinyR<span class="op">::</span><span class="kw">cluster_frequency</span>(<span class="dt">tokenized_list_text =</span> clust_vec<span class="op">$</span>token, </span>
<span id="cb15-2"><a href="#cb15-2"></a>                                               <span class="dt">cluster_vector =</span> kmed<span class="op">$</span>clusters, <span class="dt">verbose =</span> T)</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a>Time difference of <span class="fl">0.1685851</span> secs</span></code></pre></div>
<p><br></p>
<p>This is one of the ways that the transformed word-vectors can be used and is solely based on tokens (words) and word frequencies. However a more advanced approach would be to cluster documents based on word <em>n-grams</em> and take advantage of <em>graphs</em> as explained <a href="https://www.tidytextmining.com/ngrams.html#visualizing-bigrams-in-other-texts">here</a> in order to plot the nodes, edges and text.</p>
<p><br><br></p>
<p><em>References</em>:</p>
<ul>
<li><a href="https://miguelmalvarez.com/2015/03/20/classifying-reuters-21578-collection-with-python-representing-the-data/" class="uri">https://miguelmalvarez.com/2015/03/20/classifying-reuters-21578-collection-with-python-representing-the-data/</a></li>
<li>www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur</li>
<li><a href="https://erogol.com/duplicate-question-detection-deep-learning/" class="uri">https://erogol.com/duplicate-question-detection-deep-learning/</a></li>
<li><a href="https://www.tidytextmining.com/ngrams.html#visualizing-bigrams-in-other-texts" class="uri">https://www.tidytextmining.com/ngrams.html#visualizing-bigrams-in-other-texts</a></li>
</ul>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by Lampros Mouselimis.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
