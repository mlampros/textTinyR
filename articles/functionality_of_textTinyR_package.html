<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Functionality of the textTinyR package • textTinyR</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Functionality of the textTinyR package">
<meta property="og:description" content="textTinyR">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">textTinyR</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.1.4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/functionality_of_textTinyR_package.html">Functionality of the textTinyR package</a>
    </li>
    <li>
      <a href="../articles/word_vectors_doc2vec.html">Word vectors - doc2vec - text clustering</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlampros/textTinyR/">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="functionality_of_textTinyR_package_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Functionality of the textTinyR package</h1>
                        <h4 class="author">Lampros Mouselimis</h4>
            
            <h4 class="date">2021-05-16</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/mlampros/textTinyR/blob/master/vignettes/functionality_of_textTinyR_package.Rmd"><code>vignettes/functionality_of_textTinyR_package.Rmd</code></a></small>
      <div class="hidden name"><code>functionality_of_textTinyR_package.Rmd</code></div>

    </div>

    
    
<p>The following code chunks and examples show the functionality of the textTinyR package. The textTinyR package consists of the following <strong>classes</strong> (based on the R6 package) and <strong>functions</strong>: <br></p>
<div id="classes" class="section level2">
<h2 class="hasAnchor">
<a href="#classes" class="anchor"></a><strong>classes</strong>
</h2>
<table class="table">
<thead><tr class="header">
<th align="left">big_tokenize_transform</th>
<th align="left">sparse_term_matrix</th>
<th align="left">token_stats</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">big_text_splitter()</td>
<td align="left">Term_Matrix()</td>
<td align="left">path_2vector()</td>
</tr>
<tr class="even">
<td align="left">big_text_parser()</td>
<td align="left">Term_Matrix_Adjust()</td>
<td align="left">freq_distribution()</td>
</tr>
<tr class="odd">
<td align="left">big_text_tokenizer()</td>
<td align="left">term_associations()</td>
<td align="left">print_frequency()</td>
</tr>
<tr class="even">
<td align="left">vocabulary_accumulator()</td>
<td align="left">most_frequent_terms()</td>
<td align="left">count_character()</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">print_count_character()</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">collocation_words()</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">print_collocations()</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">string_dissimilarity_matrix()</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">look_up_table()</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">print_words_lookup_tbl()</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div id="functions" class="section level2">
<h2 class="hasAnchor">
<a href="#functions" class="anchor"></a><strong>functions</strong>
</h2>
<table class="table">
<thead><tr class="header">
<th align="left">sparse_matrices</th>
<th align="left">tokenization</th>
<th align="left">utilities</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">dense_2sparse()</td>
<td align="left">tokenize_transform_text()</td>
<td align="left">bytes_converter()</td>
</tr>
<tr class="even">
<td align="left">load_sparse_binary()</td>
<td align="left">tokenize_transform_vec_docs()</td>
<td align="left">cosine_distance()</td>
</tr>
<tr class="odd">
<td align="left">matrix_sparsity()</td>
<td align="left"></td>
<td align="left">dice_distance()</td>
</tr>
<tr class="even">
<td align="left">save_sparse_binary()</td>
<td align="left"></td>
<td align="left">levenshtein_distance()</td>
</tr>
<tr class="odd">
<td align="left">sparse_Means()</td>
<td align="left"></td>
<td align="left">read_characters()</td>
</tr>
<tr class="even">
<td align="left">sparse_Sums()</td>
<td align="left"></td>
<td align="left">read_rows()</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">text_file_parser()</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left">utf_locale()</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">vocabulary_parser()</td>
</tr>
</tbody>
</table>
<p><br><br></p>
</div>
<div id="big_tokenize_transform-class" class="section level2">
<h2 class="hasAnchor">
<a href="#big_tokenize_transform-class" class="anchor"></a><em>big_tokenize_transform</em> class</h2>
<p>The <em>big_tokenize_transform</em> class can be utilized to process big data files and I’ll illustrate this using the <a href="https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2">english wikipedia pages and articles</a>. The size of the file (after downloading and extracting locally) is aproximalely 59.4 GB and it’s of type .xml (to reproduce the results one needs to have free hard drive space of approx. 200 GB). <br><em>Xml</em> files have a tree structure and one should use queries to acquire specific information. First, I’ll observe the structure of the .xml file by using the utility function <em>read_rows()</em>. The <em>read_rows()</em> function takes a file as input and by specifying the <em>rows</em> argument it returns a subset of the file. It doesn’t load the entire file in memory, but it just opens the file and reads the specific number of rows,</p>
<p><br></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mlampros/textTinyR">textTinyR</a></span><span class="op">)</span>


<span class="va">PATH</span> <span class="op">=</span> <span class="st">'enwiki-latest-pages-articles.xml'</span>


<span class="va">subset</span> <span class="op">=</span> <span class="fu"><a href="../reference/read_rows.html">read_rows</a></span><span class="op">(</span>input_file <span class="op">=</span> <span class="va">PATH</span>, read_delimiter <span class="op">=</span> <span class="st">"\n"</span>,
                   
                   rows <span class="op">=</span> <span class="fl">100</span>,
                   
                   write_2file <span class="op">=</span> <span class="st">"/subs_output.txt"</span><span class="op">)</span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># data subset : subs_output.txt</span></span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="op">&lt;</span>mediawiki xmlns=<span class="st">"http://www.mediawiki.org/xml/export-0.10/"</span> xmlns<span class="op">:</span>xsi=<span class="st">"http://www.w3.org/2001/XMLSchema-instance"</span> xsi<span class="op">:</span>schemaLocation=<span class="st">"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd"</span> version=<span class="st">"0.10"</span> xml<span class="op">:</span>lang=<span class="st">"en"</span><span class="op">&gt;</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="st">  </span><span class="er">&lt;</span>siteinfo<span class="op">&gt;</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="st">    </span><span class="er">&lt;</span>sitename<span class="op">&gt;</span>Wikipedia<span class="op">&lt;</span><span class="er">/</span>sitename<span class="op">&gt;</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="st">    </span><span class="er">&lt;</span>dbname<span class="op">&gt;</span>enwiki<span class="op">&lt;</span><span class="er">/</span>dbname<span class="op">&gt;</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="st">    </span><span class="er">&lt;</span>base<span class="op">&gt;</span>https<span class="op">:</span><span class="er">//</span>en.wikipedia.org<span class="op">/</span>wiki<span class="op">/</span>Main_Page<span class="op">&lt;</span><span class="er">/</span>base<span class="op">&gt;</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="st">    </span><span class="er">&lt;</span>generator<span class="op">&gt;</span>MediaWiki <span class="dv">1</span>.<span class="fl">28.0</span><span class="op">-</span>wmf<span class="fl">.23</span><span class="op">&lt;</span><span class="er">/</span>generator<span class="op">&gt;</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="st">    </span><span class="er">&lt;</span>case<span class="op">&gt;</span>first<span class="op">-</span>letter<span class="op">&lt;</span><span class="er">/</span>case<span class="op">&gt;</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="st">    </span><span class="er">&lt;</span>namespaces<span class="op">&gt;</span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"-2"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>Media<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"-1"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>Special<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"0"</span> case=<span class="st">"first-letter"</span> <span class="op">/</span><span class="er">&gt;</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"1"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>Talk<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"2"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>User<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"3"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>User talk<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"4"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>Wikipedia<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"5"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>Wikipedia talk<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"6"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>File<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"7"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>File talk<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="st">      </span><span class="er">&lt;</span>namespace key=<span class="st">"8"</span> case=<span class="st">"first-letter"</span><span class="op">&gt;</span>MediaWiki<span class="op">&lt;</span><span class="er">/</span>namespace<span class="op">&gt;</span></span>
<span id="cb2-23"><a href="#cb2-23"></a>.</span>
<span id="cb2-24"><a href="#cb2-24"></a>.</span>
<span id="cb2-25"><a href="#cb2-25"></a>.</span>
<span id="cb2-26"><a href="#cb2-26"></a>    <span class="op">&lt;</span><span class="er">/</span>namespaces<span class="op">&gt;</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="st">  </span><span class="er">&lt;/</span>siteinfo<span class="op">&gt;</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="st">  </span><span class="er">&lt;</span>page<span class="op">&gt;</span></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="st">    </span><span class="er">&lt;</span>title<span class="op">&gt;</span>AccessibleComputing<span class="op">&lt;</span><span class="er">/</span>title<span class="op">&gt;</span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="st">    </span><span class="er">&lt;</span>ns<span class="op">&gt;</span><span class="dv">0</span><span class="op">&lt;</span><span class="er">/</span>ns<span class="op">&gt;</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="st">    </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">10</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="st">    </span><span class="er">&lt;</span>redirect title=<span class="st">"Computer accessibility"</span> <span class="op">/</span><span class="er">&gt;</span></span>
<span id="cb2-33"><a href="#cb2-33"></a><span class="st">    </span><span class="er">&lt;</span>revision<span class="op">&gt;</span></span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="st">      </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">631144794</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-35"><a href="#cb2-35"></a><span class="st">      </span><span class="er">&lt;</span>parentid<span class="op">&gt;</span><span class="dv">381202555</span><span class="op">&lt;</span><span class="er">/</span>parentid<span class="op">&gt;</span></span>
<span id="cb2-36"><a href="#cb2-36"></a><span class="st">      </span><span class="er">&lt;</span>timestamp<span class="op">&gt;</span><span class="dv">2014-10</span><span class="op">-</span>26T04<span class="op">:</span><span class="dv">50</span><span class="op">:</span>23Z<span class="op">&lt;</span><span class="er">/</span>timestamp<span class="op">&gt;</span></span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="st">      </span><span class="er">&lt;</span>contributor<span class="op">&gt;</span></span>
<span id="cb2-38"><a href="#cb2-38"></a><span class="st">        </span><span class="er">&lt;</span>username<span class="op">&gt;</span>Paine Ellsworth<span class="op">&lt;</span><span class="er">/</span>username<span class="op">&gt;</span></span>
<span id="cb2-39"><a href="#cb2-39"></a><span class="st">        </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">9092818</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="st">      </span><span class="er">&lt;/</span>contributor<span class="op">&gt;</span></span>
<span id="cb2-41"><a href="#cb2-41"></a><span class="st">      </span><span class="er">&lt;</span>comment<span class="op">&gt;</span>add [[WP<span class="op">:</span>RCAT<span class="op">|</span>rcat]]s<span class="op">&lt;</span><span class="er">/</span>comment<span class="op">&gt;</span></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="st">      </span><span class="er">&lt;</span>model<span class="op">&gt;</span>wikitext<span class="op">&lt;</span><span class="er">/</span>model<span class="op">&gt;</span></span>
<span id="cb2-43"><a href="#cb2-43"></a><span class="st">      </span><span class="er">&lt;</span>format<span class="op">&gt;</span>text<span class="op">/</span>x<span class="op">-</span>wiki<span class="op">&lt;</span><span class="er">/</span>format<span class="op">&gt;</span></span>
<span id="cb2-44"><a href="#cb2-44"></a><span class="st">      </span><span class="er">&lt;</span>text xml<span class="op">:</span>space=<span class="st">"preserve"</span><span class="op">&gt;</span><span class="co">#REDIRECT [[Computer accessibility]]</span></span>
<span id="cb2-45"><a href="#cb2-45"></a></span>
<span id="cb2-46"><a href="#cb2-46"></a>{{Redr<span class="op">|</span>move<span class="op">|</span>from CamelCase<span class="op">|</span>up}}<span class="op">&lt;</span><span class="er">/</span>text<span class="op">&gt;</span></span>
<span id="cb2-47"><a href="#cb2-47"></a><span class="st">      </span><span class="er">&lt;</span>sha1<span class="op">&gt;</span>4ro7vvppa5kmm0o1egfjztzcwd0vabw<span class="op">&lt;</span><span class="er">/</span>sha1<span class="op">&gt;</span></span>
<span id="cb2-48"><a href="#cb2-48"></a><span class="st">    </span><span class="er">&lt;/</span>revision<span class="op">&gt;</span></span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="st">  </span><span class="er">&lt;/</span>page<span class="op">&gt;</span></span>
<span id="cb2-50"><a href="#cb2-50"></a><span class="st">  </span><span class="er">&lt;</span>page<span class="op">&gt;</span></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="st">    </span><span class="er">&lt;</span>title<span class="op">&gt;</span>Anarchism<span class="op">&lt;</span><span class="er">/</span>title<span class="op">&gt;</span></span>
<span id="cb2-52"><a href="#cb2-52"></a><span class="st">    </span><span class="er">&lt;</span>ns<span class="op">&gt;</span><span class="dv">0</span><span class="op">&lt;</span><span class="er">/</span>ns<span class="op">&gt;</span></span>
<span id="cb2-53"><a href="#cb2-53"></a><span class="st">    </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">12</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-54"><a href="#cb2-54"></a><span class="st">    </span><span class="er">&lt;</span>revision<span class="op">&gt;</span></span>
<span id="cb2-55"><a href="#cb2-55"></a><span class="st">      </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">746687538</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-56"><a href="#cb2-56"></a><span class="st">      </span><span class="er">&lt;</span>parentid<span class="op">&gt;</span><span class="dv">744318951</span><span class="op">&lt;</span><span class="er">/</span>parentid<span class="op">&gt;</span></span>
<span id="cb2-57"><a href="#cb2-57"></a><span class="st">      </span><span class="er">&lt;</span>timestamp<span class="op">&gt;</span><span class="dv">2016-10</span><span class="op">-</span>28T22<span class="op">:</span><span class="dv">43</span><span class="op">:</span>19Z<span class="op">&lt;</span><span class="er">/</span>timestamp<span class="op">&gt;</span></span>
<span id="cb2-58"><a href="#cb2-58"></a><span class="st">      </span><span class="er">&lt;</span>contributor<span class="op">&gt;</span></span>
<span id="cb2-59"><a href="#cb2-59"></a><span class="st">        </span><span class="er">&lt;</span>username<span class="op">&gt;</span>Eduen<span class="op">&lt;</span><span class="er">/</span>username<span class="op">&gt;</span></span>
<span id="cb2-60"><a href="#cb2-60"></a><span class="st">        </span><span class="er">&lt;</span>id<span class="op">&gt;</span><span class="dv">7527773</span><span class="op">&lt;</span><span class="er">/</span>id<span class="op">&gt;</span></span>
<span id="cb2-61"><a href="#cb2-61"></a><span class="st">      </span><span class="er">&lt;/</span>contributor<span class="op">&gt;</span></span>
<span id="cb2-62"><a href="#cb2-62"></a><span class="st">      </span><span class="er">&lt;</span>minor <span class="op">/</span><span class="er">&gt;</span></span>
<span id="cb2-63"><a href="#cb2-63"></a><span class="st">      </span><span class="er">&lt;</span>comment<span class="op">&gt;</span><span class="er">/*</span><span class="st"> </span>Free love <span class="op">*</span><span class="er">/&lt;/</span>comment<span class="op">&gt;</span></span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="st">      </span><span class="er">&lt;</span>model<span class="op">&gt;</span>wikitext<span class="op">&lt;</span><span class="er">/</span>model<span class="op">&gt;</span></span>
<span id="cb2-65"><a href="#cb2-65"></a><span class="st">      </span><span class="er">&lt;</span>format<span class="op">&gt;</span>text<span class="op">/</span>x<span class="op">-</span>wiki<span class="op">&lt;</span><span class="er">/</span>format<span class="op">&gt;</span></span>
<span id="cb2-66"><a href="#cb2-66"></a><span class="st">      </span><span class="er">&lt;</span>text xml<span class="op">:</span>space=<span class="st">"preserve"</span><span class="op">&gt;</span>{{Redirect2<span class="op">|</span>Anarchist<span class="op">|</span>Anarchists<span class="op">|</span>the fictional character<span class="op">|</span><span class="kw">Anarchist</span> (comics)<span class="op">|</span>other uses<span class="op">|</span><span class="kw">Anarchists</span> (disambiguation)}}</span>
<span id="cb2-67"><a href="#cb2-67"></a>{{pp<span class="op">-</span>move<span class="op">-</span>indef}}</span>
<span id="cb2-68"><a href="#cb2-68"></a>.</span>
<span id="cb2-69"><a href="#cb2-69"></a>.</span>
<span id="cb2-70"><a href="#cb2-70"></a>.</span></code></pre></div>
<p><br></p>
<p>In that way one has a picture of the .xml tree structure and can continue by performing queries. The initial data file is too big to fit in the memory of a PC, thus it has to be split in smaller files, pre-processed and then returned as a single file. The main aim of the <em>big_text_splitter()</em> method is to split the data in smaller files of (approx.) equal size by either using the <em>batches</em> parameter or if the file has a structure by adding the <em>end_query</em> parameter too. Here I’ll take advantage of both the <em>batches</em> and the <em>end_query</em> parameters for this task, because I’ll use queries to extract the text tree-elements, so I don’t want that the file is split arbitrarily. Each sub-element in the file begins and ends with the same key-word, i.e. text,</p>
<p><br></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">btt</span> <span class="op">=</span> <span class="va"><a href="../reference/big_tokenize_transform.html">big_tokenize_transform</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">btt</span><span class="op">$</span><span class="fu">big_text_splitter</span><span class="op">(</span>input_path_file <span class="op">=</span> <span class="va">PATH</span>,             <span class="co"># path to the enwiki data file</span>
                  
                  output_path_folder <span class="op">=</span> <span class="st">"/enwiki_spl_data/"</span>,  <span class="co"># folder to save the files</span>
                  
                  end_query <span class="op">=</span> <span class="st">'&lt;/text&gt;'</span>,    <span class="co"># splits the file taking into account the key-word</span>
                  
                  batches <span class="op">=</span> <span class="fl">40</span>,                           <span class="co"># split file in 40 batches (files)</span>
                  
                  trimmed_line <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>                   <span class="co"># the lines will be trimmed</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>approx. <span class="dv">10</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-2"><a href="#cb4-2"></a>approx. <span class="dv">20</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-3"><a href="#cb4-3"></a>approx. <span class="dv">30</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-4"><a href="#cb4-4"></a>approx. <span class="dv">40</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-5"><a href="#cb4-5"></a>approx. <span class="dv">50</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-6"><a href="#cb4-6"></a>approx. <span class="dv">60</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-7"><a href="#cb4-7"></a>approx. <span class="dv">70</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-8"><a href="#cb4-8"></a>approx. <span class="dv">80</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-9"><a href="#cb4-9"></a>approx. <span class="dv">90</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-10"><a href="#cb4-10"></a>approx. <span class="dv">100</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb4-11"><a href="#cb4-11"></a></span>
<span id="cb4-12"><a href="#cb4-12"></a>It took <span class="fl">42.7098</span> minutes to complete the splitting</span></code></pre></div>
<p><br></p>
<p>After the data is split and saved in the <em>output_path_folder</em> (“/ewiki_spl_data/”) the next step is to extract the <strong>text</strong> tree-elements from the batches by using the <em>big_text_parser()</em> method. The latter takes as arguments the previously created <em>input_path_folder</em>, an <em>output_path_folder</em> to save the resulted text files, a <em>start_query</em>, an <em>end_query</em>, the <em>min_lines</em> (only subsets of text with more than or equal to this minimum will be kept) and the <em>trimmed_line</em> ( specifying if each line is already trimmed both-sides ),</p>
<p><br></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">btt</span><span class="op">$</span><span class="fu">big_text_parser</span><span class="op">(</span>input_path_folder <span class="op">=</span> <span class="st">"/enwiki_spl_data/"</span>, <span class="co"># the previously created folder</span>
                    
                    output_path_folder <span class="op">=</span> <span class="st">"/enwiki_parse/"</span>,  <span class="co"># folder to save the parsed files</span>
                    
                    start_query <span class="op">=</span> <span class="st">"&lt;text xml:space=\"preserve\"&gt;"</span>,  <span class="co"># starts to extract text</span>
                    
                    end_query <span class="op">=</span> <span class="st">"&lt;/text&gt;"</span>,                        <span class="co"># stop to extract once here</span>
                    
                    min_lines <span class="op">=</span> <span class="fl">1</span>, 
                    
                    trimmed_line <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="op">==</span><span class="er">==================</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>batch <span class="dv">1</span> begins ...</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="op">==</span><span class="er">==================</span></span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>approx. <span class="dv">10</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-6"><a href="#cb6-6"></a>approx. <span class="dv">20</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-7"><a href="#cb6-7"></a>approx. <span class="dv">30</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-8"><a href="#cb6-8"></a>approx. <span class="dv">40</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-9"><a href="#cb6-9"></a>approx. <span class="dv">50</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-10"><a href="#cb6-10"></a>approx. <span class="dv">60</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-11"><a href="#cb6-11"></a>approx. <span class="dv">70</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-12"><a href="#cb6-12"></a>approx. <span class="dv">80</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-13"><a href="#cb6-13"></a>approx. <span class="dv">90</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-14"><a href="#cb6-14"></a>approx. <span class="dv">100</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a>It took <span class="fl">0.296151</span> minutes to complete the preprocessing</span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a>It took <span class="fl">0.0525948</span> minutes to save the pre<span class="op">-</span>processed data</span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>.</span>
<span id="cb6-21"><a href="#cb6-21"></a>.</span>
<span id="cb6-22"><a href="#cb6-22"></a>.</span>
<span id="cb6-23"><a href="#cb6-23"></a>.</span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a><span class="op">==</span><span class="er">==================</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>batch <span class="dv">40</span> begins ...</span>
<span id="cb6-27"><a href="#cb6-27"></a><span class="op">==</span><span class="er">==================</span></span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>approx. <span class="dv">10</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-30"><a href="#cb6-30"></a>approx. <span class="dv">20</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-31"><a href="#cb6-31"></a>approx. <span class="dv">30</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-32"><a href="#cb6-32"></a>approx. <span class="dv">40</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-33"><a href="#cb6-33"></a>approx. <span class="dv">50</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-34"><a href="#cb6-34"></a>approx. <span class="dv">60</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-35"><a href="#cb6-35"></a>approx. <span class="dv">70</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-36"><a href="#cb6-36"></a>approx. <span class="dv">80</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-37"><a href="#cb6-37"></a>approx. <span class="dv">90</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-38"><a href="#cb6-38"></a>approx. <span class="dv">100</span> % of data pre<span class="op">-</span>processed</span>
<span id="cb6-39"><a href="#cb6-39"></a></span>
<span id="cb6-40"><a href="#cb6-40"></a>It took <span class="fl">1.04127</span> minutes to complete the preprocessing</span>
<span id="cb6-41"><a href="#cb6-41"></a></span>
<span id="cb6-42"><a href="#cb6-42"></a>It took <span class="fl">0.0448579</span> minutes to save the pre<span class="op">-</span>processed data</span>
<span id="cb6-43"><a href="#cb6-43"></a></span>
<span id="cb6-44"><a href="#cb6-44"></a>It took <span class="fl">40.9034</span> minutes to complete the parsing</span></code></pre></div>
<p><br></p>
<p>Here, it’s worth mentioning that the <em>big_text_parser</em> is more efficient if it extracts big chunks of text, rather than one-liners. In case of one-line text queries it has to check line by line the whole file, which is inefficient especially for files equal to the enwiki size.</p>
<p><br></p>
<p>By extracting the text chunks from the data the .xml file size reduces to (approx.) 48.9 GB. One can now continue utilizing the <em>big_text_tokenizer()</em> method in order to tokenize and transform the data. This method takes the following parameters:</p>
<p><strong>batches</strong> (each file can be further split in batches during tokenization), <strong>to_lower</strong> (convert to lower case), <strong>to_upper</strong> (convert to upper case), <strong>utf_locale</strong> (change utf locale depending on the language), <strong>remove_char</strong> (remove specific characters from the text), <strong>remove_punctuation_string</strong> (remove punctuation before the data is split), <strong>remove_punctuation_vector</strong> (remove punctuation after the data is split), <strong>remove_numbers</strong> (remove numbers from the data), <strong>trim_token</strong> (trim the tokens both-sides), <strong>split_string</strong> (split the string), <strong>split_separator</strong> (token split seprator where multiple delimiters can be used), <strong>remove_stopwords</strong> (remove stopwords using one of the available languages or by providing a user defined vector of words), <strong>language</strong> (the language of use), <strong>min_num_char</strong> (the minimum number of characters to keep), <strong>max_num_char</strong> (the maximum number of characters to keep), <strong>stemmer</strong> (stemming of the words using either the porter_2steemer or n-gram stemming – those two methods will be explained in the tokenization function), <strong>min_n_gram</strong> (minimum n-grams), <strong>max_n_gram</strong> (maximum n-grams), <strong>skip_n_gram</strong> (skip n-gram), <strong>skip_distance</strong> (skip distance for n-grams), <strong>n_gram_delimiter</strong> (n-gram delimiter), <strong>concat_delimiter</strong> (concatenation of the data in case that one wants to save the file), <strong>path_2folder</strong> (specified folder to save the data), <strong>stemmer_ngram</strong> (in case of n-gram stemming the n-grams), <strong>stemmer_gamma</strong> (in case of n-gram stemming the gamma parameter), <strong>stemmer_truncate</strong> (in case of n-gram stemming the truncation parameter), <strong>stemmer_batches</strong> (in case of n-gram stemming the batches parameter ), <strong>threads</strong> (the number of cores to use in parallel ), <strong>save_2single_file</strong> (should the output data be saved in a single file), <strong>increment_batch_nr</strong> (the enumeration of the output files will start from this number), <strong>vocabulary_path_file</strong> (should a vocabulary be saved in a separate file). <br></p>
<p>More information about those parameters can be found in the package documentation.</p>
<p><br></p>
<p>In this vignette I’ll continue using the following transformations:</p>
<ul>
<li>conversion to lowercase</li>
<li>trim each line</li>
<li>split each line using multiple delimiters</li>
<li>remove the punctuation ( once splitting is taken place )</li>
<li>remove the numbers from the tokens</li>
<li>limit the output words to a specific number of characters</li>
<li>remove the english stopwords</li>
<li>and save both the data (to a single file) and the vocabulary files (to a folder).</li>
</ul>
<p>Each initial file will be split in additional batches to limit the memory usage during the tokenization and transformation phase,</p>
<p><br></p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">btt</span><span class="op">$</span><span class="fu">big_text_tokenizer</span><span class="op">(</span>input_path_folder <span class="op">=</span> <span class="st">"/enwiki_parse/"</span>,   <span class="co"># the previously parsed data</span>
                       
                       batches <span class="op">=</span> <span class="fl">4</span>,     <span class="co"># each single file will be split further in 4 batches</span>
                       
                       to_lower <span class="op">=</span> <span class="cn">TRUE</span>, trim_token <span class="op">=</span> <span class="cn">TRUE</span>,
                       
                       split_string<span class="op">=</span><span class="cn">TRUE</span>, max_num_char <span class="op">=</span> <span class="fl">100</span>,
                       
                       split_separator <span class="op">=</span> <span class="st">" \r\n\t.,;:()?!//"</span>,
                       
                       remove_punctuation_vector <span class="op">=</span> <span class="cn">TRUE</span>,
                       
                       remove_numbers <span class="op">=</span> <span class="cn">TRUE</span>,
                       
                       remove_stopwords <span class="op">=</span> <span class="cn">TRUE</span>,                
                       
                       threads <span class="op">=</span> <span class="fl">4</span>, 
                       
                       save_2single_file <span class="op">=</span> <span class="cn">TRUE</span>,      <span class="co"># save to a single file</span>
                       
                       vocabulary_path_folder <span class="op">=</span> <span class="st">"/enwiki_vocab/"</span>,  <span class="co"># path to vocabulary folder</span>
                       
                       path_2folder<span class="op">=</span><span class="st">"/enwiki_token/"</span><span class="op">)</span>   <span class="co"># folder to save the transformed data</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="op">==</span><span class="er">==================================</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>transformation of file <span class="dv">1</span> starts ...</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="op">==</span><span class="er">==================================</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="op">-------------------</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>batch <span class="dv">1</span> begins ...</span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="op">-------------------</span></span>
<span id="cb8-8"><a href="#cb8-8"></a></span>
<span id="cb8-9"><a href="#cb8-9"></a>input of the data starts ...</span>
<span id="cb8-10"><a href="#cb8-10"></a>conversion to lower case starts ...</span>
<span id="cb8-11"><a href="#cb8-11"></a>removal of numeric values starts ...</span>
<span id="cb8-12"><a href="#cb8-12"></a>the string<span class="op">-</span>trim starts ...</span>
<span id="cb8-13"><a href="#cb8-13"></a>the split of the character string and simultaneously the removal of the punctuation <span class="cf">in</span> the vector starts ...</span>
<span id="cb8-14"><a href="#cb8-14"></a>stop words of the english language will be used</span>
<span id="cb8-15"><a href="#cb8-15"></a>the removal of stop<span class="op">-</span>words starts ...</span>
<span id="cb8-16"><a href="#cb8-16"></a>character strings with more than or equal to <span class="dv">1</span> and less than <span class="dv">100</span> characters will be kept ...</span>
<span id="cb8-17"><a href="#cb8-17"></a>the vocabulary counts will be saved <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_vocab<span class="op">/</span>batch1.txt</span>
<span id="cb8-18"><a href="#cb8-18"></a>the pre<span class="op">-</span>processed data will be saved <span class="cf">in</span> a single file <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_token<span class="op">/</span></span>
<span id="cb8-19"><a href="#cb8-19"></a></span>
<span id="cb8-20"><a href="#cb8-20"></a><span class="op">-------------------</span></span>
<span id="cb8-21"><a href="#cb8-21"></a>batch <span class="dv">2</span> begins ...</span>
<span id="cb8-22"><a href="#cb8-22"></a><span class="op">-------------------</span></span>
<span id="cb8-23"><a href="#cb8-23"></a></span>
<span id="cb8-24"><a href="#cb8-24"></a>input of the data starts ...</span>
<span id="cb8-25"><a href="#cb8-25"></a>conversion to lower case starts ...</span>
<span id="cb8-26"><a href="#cb8-26"></a>removal of numeric values starts ...</span>
<span id="cb8-27"><a href="#cb8-27"></a>the string<span class="op">-</span>trim starts ...</span>
<span id="cb8-28"><a href="#cb8-28"></a>the split of the character string and simultaneously the removal of the punctuation <span class="cf">in</span> the vector starts ...</span>
<span id="cb8-29"><a href="#cb8-29"></a>stop words of the english language will be used</span>
<span id="cb8-30"><a href="#cb8-30"></a>the removal of stop<span class="op">-</span>words starts ...</span>
<span id="cb8-31"><a href="#cb8-31"></a>character strings with more than or equal to <span class="dv">1</span> and less than <span class="dv">100</span> characters will be kept ...</span>
<span id="cb8-32"><a href="#cb8-32"></a>the vocabulary counts will be saved <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_vocab<span class="op">/</span>batch1.txt</span>
<span id="cb8-33"><a href="#cb8-33"></a>the pre<span class="op">-</span>processed data will be saved <span class="cf">in</span> a single file <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_token<span class="op">/</span></span>
<span id="cb8-34"><a href="#cb8-34"></a><span class="st">  </span></span>
<span id="cb8-35"><a href="#cb8-35"></a>.</span>
<span id="cb8-36"><a href="#cb8-36"></a>.</span>
<span id="cb8-37"><a href="#cb8-37"></a>.</span>
<span id="cb8-38"><a href="#cb8-38"></a>.</span>
<span id="cb8-39"><a href="#cb8-39"></a></span>
<span id="cb8-40"><a href="#cb8-40"></a><span class="op">==</span><span class="er">==================================</span></span>
<span id="cb8-41"><a href="#cb8-41"></a>transformation of file <span class="dv">40</span> starts ...</span>
<span id="cb8-42"><a href="#cb8-42"></a><span class="op">==</span><span class="er">==================================</span></span>
<span id="cb8-43"><a href="#cb8-43"></a></span>
<span id="cb8-44"><a href="#cb8-44"></a><span class="op">-------------------</span></span>
<span id="cb8-45"><a href="#cb8-45"></a>batch <span class="dv">1</span> begins ...</span>
<span id="cb8-46"><a href="#cb8-46"></a><span class="op">-------------------</span></span>
<span id="cb8-47"><a href="#cb8-47"></a></span>
<span id="cb8-48"><a href="#cb8-48"></a>input of the data starts ...</span>
<span id="cb8-49"><a href="#cb8-49"></a>conversion to lower case starts ...</span>
<span id="cb8-50"><a href="#cb8-50"></a>removal of numeric values starts ...</span>
<span id="cb8-51"><a href="#cb8-51"></a>the string<span class="op">-</span>trim starts ...</span>
<span id="cb8-52"><a href="#cb8-52"></a>the split of the character string and simultaneously the removal of the punctuation <span class="cf">in</span> the vector starts ...</span>
<span id="cb8-53"><a href="#cb8-53"></a>stop words of the english language will be used</span>
<span id="cb8-54"><a href="#cb8-54"></a>the removal of stop<span class="op">-</span>words starts ...</span>
<span id="cb8-55"><a href="#cb8-55"></a>character strings with more than or equal to <span class="dv">1</span> and less than <span class="dv">100</span> characters will be kept ...</span>
<span id="cb8-56"><a href="#cb8-56"></a>the vocabulary counts will be saved <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_vocab<span class="op">/</span>batch40.txt</span>
<span id="cb8-57"><a href="#cb8-57"></a>the pre<span class="op">-</span>processed data will be saved <span class="cf">in</span> a single file <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_token<span class="op">/</span></span>
<span id="cb8-58"><a href="#cb8-58"></a></span>
<span id="cb8-59"><a href="#cb8-59"></a><span class="op">-------------------</span></span>
<span id="cb8-60"><a href="#cb8-60"></a>batch <span class="dv">2</span> begins ...</span>
<span id="cb8-61"><a href="#cb8-61"></a><span class="op">-------------------</span></span>
<span id="cb8-62"><a href="#cb8-62"></a></span>
<span id="cb8-63"><a href="#cb8-63"></a>input of the data starts ...</span>
<span id="cb8-64"><a href="#cb8-64"></a>conversion to lower case starts ...</span>
<span id="cb8-65"><a href="#cb8-65"></a>removal of numeric values starts ...</span>
<span id="cb8-66"><a href="#cb8-66"></a>the string<span class="op">-</span>trim starts ...</span>
<span id="cb8-67"><a href="#cb8-67"></a>the split of the character string and simultaneously the removal of the punctuation <span class="cf">in</span> the vector starts ...</span>
<span id="cb8-68"><a href="#cb8-68"></a>stop words of the english language will be used</span>
<span id="cb8-69"><a href="#cb8-69"></a>the removal of stop<span class="op">-</span>words starts ...</span>
<span id="cb8-70"><a href="#cb8-70"></a>character strings with more than or equal to <span class="dv">1</span> and less than <span class="dv">100</span> characters will be kept ...</span>
<span id="cb8-71"><a href="#cb8-71"></a>the vocabulary counts will be saved <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_vocab<span class="op">/</span>batch40.txt</span>
<span id="cb8-72"><a href="#cb8-72"></a>the pre<span class="op">-</span>processed data will be saved <span class="cf">in</span> a single file <span class="cf">in</span><span class="op">:</span><span class="st"> </span><span class="er">/</span>enwiki_token<span class="op">/</span></span>
<span id="cb8-73"><a href="#cb8-73"></a><span class="st">  </span></span>
<span id="cb8-74"><a href="#cb8-74"></a>.</span>
<span id="cb8-75"><a href="#cb8-75"></a>.</span>
<span id="cb8-76"><a href="#cb8-76"></a>.</span>
<span id="cb8-77"><a href="#cb8-77"></a>.</span>
<span id="cb8-78"><a href="#cb8-78"></a></span>
<span id="cb8-79"><a href="#cb8-79"></a>It took <span class="fl">111.689</span> minutes to complete tokenization</span></code></pre></div>
<p><br></p>
<p>In total, it took approx. 195 minutes (or 3.25 hours) to pre-process (including tokenization, transformation and vocabulary extraction) the 59.4 GB of the enwiki data. <br></p>
<p><br></p>
<div id="word-cloud" class="section level4">
<h4 class="hasAnchor">
<a href="#word-cloud" class="anchor"></a><em>word cloud</em>
</h4>
<p>Having a clean single text file of all the wikipedia pages and articles one can perform many tasks. For instance, one can build a <em>wordcloud</em> (using the wordcloud package) from the accumulated words (a word of caution : the memory consumption when running the <em>vocabulary_accumulator</em> method for this kind of data size can exceed the 10 GB),</p>
<p><br></p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span><span class="op">$</span><span class="fu">vocabulary_accumulator</span><span class="op">(</span>input_path_folder <span class="op">=</span> <span class="st">"/enwiki_vocab/"</span>, 
                            
                            vocabulary_path_file <span class="op">=</span> <span class="st">"/VOCAB.txt"</span>,
                            
                            max_num_chars <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>vocabulary.of.batch <span class="dv">40</span> will.be.merged ...   minutes.to.merge.sort.batches<span class="op">:</span><span class="st"> </span><span class="fl">4.57273</span></span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a>    minutes.to.save.data<span class="op">:</span><span class="st"> </span><span class="fl">0.48584</span></span></code></pre></div>
<p><br></p>
<p>The following table shows the first rows of the vocabulary counts,</p>
<p><br></p>
<table class="table">
<thead><tr class="header">
<th align="left">terms</th>
<th align="right">frequency</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">lt</td>
<td align="right">111408435</td>
</tr>
<tr class="even">
<td align="left">refgt</td>
<td align="right">49197149</td>
</tr>
<tr class="odd">
<td align="left">quot</td>
<td align="right">48688082</td>
</tr>
<tr class="even">
<td align="left">gt</td>
<td align="right">47466149</td>
</tr>
<tr class="odd">
<td align="left">user</td>
<td align="right">32042007</td>
</tr>
<tr class="even">
<td align="left">category</td>
<td align="right">30619748</td>
</tr>
<tr class="odd">
<td align="left">www</td>
<td align="right">25358252</td>
</tr>
<tr class="even">
<td align="left">http</td>
<td align="right">23008243</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>before plotting the wordcloud, I’ll limit the vocabulary to the first 200 words,</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">rdr_vocab</span> <span class="op">=</span> <span class="fu">textTinyR</span><span class="fu">::</span><span class="fu"><a href="../reference/read_rows.html">read_rows</a></span><span class="op">(</span>input_file <span class="op">=</span> <span class="st">"/VOCAB.txt"</span>, read_delimiter <span class="op">=</span> <span class="st">"\n"</span>,
                                 
                                 rows <span class="op">=</span> <span class="fl">200</span>, 
                                 
                                 write_2file <span class="op">=</span> <span class="st">"/vocab_subset_200terms.txt"</span><span class="op">)</span> 

<span class="co"># read the reduced data </span>

<span class="va">vocab_sbs</span> <span class="op">&lt;-</span> <span class="fu">readr</span><span class="fu">::</span><span class="fu">read_delim</span><span class="op">(</span><span class="st">"/vocab_subset_200terms.txt"</span>, <span class="st">"\t"</span>,
                               
                               escape_double <span class="op">=</span> <span class="cn">FALSE</span>, col_names <span class="op">=</span> <span class="cn">FALSE</span>, 
                               
                               trim_ws <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># create the wordcloud</span>

<span class="va">pal2</span> <span class="op">&lt;-</span> <span class="fu">RColorBrewer</span><span class="fu">::</span><span class="fu">brewer.pal</span><span class="op">(</span><span class="fl">8</span>, <span class="st">"Dark2"</span><span class="op">)</span>

<span class="fu">wordcloud</span><span class="fu">::</span><span class="fu">wordcloud</span><span class="op">(</span>words <span class="op">=</span> <span class="va">vocab_sbs</span><span class="op">$</span><span class="va">X1</span>, freq <span class="op">=</span> <span class="va">vocab_sbs</span><span class="op">$</span><span class="va">X2</span>, 
                     
                     scale <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4.5</span>, <span class="fl">0.8</span><span class="op">)</span>, random.order <span class="op">=</span> <span class="cn">FALSE</span>, 
                     
                     rot.per <span class="op">=</span> <span class="fl">.15</span>, colors <span class="op">=</span> <span class="va">pal2</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p><img src="Rplot.png"></p>
<p><br></p>
<p><br></p>
</div>
<div id="word-vectors" class="section level4">
<h4 class="hasAnchor">
<a href="#word-vectors" class="anchor"></a><em>word vectors</em>
</h4>
<p><br></p>
<p><strong>UPDATE 11-04-2019</strong>: There is an <a href="https://github.com/mlampros/fastText">updated version of the fastText R package</a> which includes all the features of the ported <a href="https://github.com/facebookresearch/fastText">fasttext library</a>. Therefore the old <strong>fastTextR</strong> repository <strong>is archived</strong>. See also the corresponding <a href="http://mlampros.github.io/2019/04/11/fastText_updated_version/">blog-post</a>.</p>
<p><br></p>
<p>I’m currently also interested in word vectors and that’s why I also made R-wrappers for the <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a> and the <a href="https://github.com/facebookresearch/fastText">fastText</a> word representation algorithms. The latter two reside in my Github account as separate repositories (<a href="https://github.com/mlampros/GloveR">GloveR</a> and <a href="https://github.com/mlampros/fastTextR">fastTextR</a>) and can be installed using the <em>install_github</em> function of the devtools package (devtools::install_github(‘mlampros/GloveR’), devtools::install_github(‘mlampros/fastTextR’)). <br> “A <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/turian-wordrepresentations-acl10.pdf">word representation</a> is a mathematical object associated with each word, often a vector. Each dimension’s value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature. Conventionally, supervised lexicalized NLP approaches take a word and convert it to a symbolic ID, which is then transformed into a feature vector using a one-hot representation: The feature vector has the same length as the size of the vocabulary, and only one dimension is on.” <br></p>
<p>Currently, there are many resources on the web on how to use <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">pre-trained word vectors (embeddings) as input to neural networks</a>.</p>
<p>In this vignette I’ll use only the <em>fastTextR</em> word representation algorithm, however detailed documentation and system requirements on how to use either the <em>GloveR</em> or the <em>fastTextR</em> can be found in the corresponding repository. If I would train the whole data file (32.2 GB) using the <em>fastTextR</em> wrapper, it would take (approx.) 15 hours,</p>
<p><br></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">fastTextR</span><span class="op">)</span>

<span class="va">skp</span> <span class="op">=</span> <span class="fu">skipgram_cbow</span><span class="op">(</span>input_path <span class="op">=</span> <span class="st">"/output_token_single_file.txt"</span>, thread <span class="op">=</span> <span class="fl">4</span>, dim <span class="op">=</span> <span class="fl">50</span>,
                    
                    output_path <span class="op">=</span> <span class="st">"/model"</span>, method <span class="op">=</span> <span class="st">"skipgram"</span>, verbose <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>Read 4018M words</span>
<span id="cb13-2"><a href="#cb13-2"></a>Number of words<span class="op">:</span><span class="st">  </span><span class="dv">12827221</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>Number of labels<span class="op">:</span><span class="st"> </span><span class="dv">0</span></span>
<span id="cb13-4"><a href="#cb13-4"></a>Progress<span class="op">:</span><span class="st"> </span><span class="fl">0.2</span>%  words<span class="op">/</span>sec<span class="op">/</span>thread<span class="op">:</span><span class="st"> </span><span class="dv">89664</span>  lr<span class="op">:</span><span class="st"> </span><span class="fl">0.099841</span>  loss<span class="op">:</span><span class="st"> </span><span class="fl">1.055581</span>  eta<span class="op">:</span><span class="st"> </span>15h32m  14m</span></code></pre></div>
<p><br></p>
<p>thus, just for illustration purposes I’ll limit the train data to approx. 1 GB of the output file,</p>
<p><br></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">reduced_data</span> <span class="op">=</span> <span class="fu"><a href="../reference/read_characters.html">read_characters</a></span><span class="op">(</span>input_file <span class="op">=</span> <span class="st">"/output_token_single_file.txt"</span>, 
                               
                               characters <span class="op">=</span> <span class="fl">1000000000</span>,        <span class="co"># approx. 1 GB of the data</span>
                               
                               write_2file <span class="op">=</span> <span class="st">"/reduced_single_file.txt"</span><span class="op">)</span>




<span class="va">skp</span> <span class="op">=</span> <span class="fu">skipgram_cbow</span><span class="op">(</span>input_path <span class="op">=</span> <span class="st">"/reduced_single_file.txt"</span>,  <span class="co"># reduced data set</span>
                    
                    output_path <span class="op">=</span> <span class="st">"/model"</span>,                <span class="co"># saves model and word vectors</span>
                    
                    dim <span class="op">=</span> <span class="fl">50</span>,                              <span class="co"># 50-dimensional word vectors</span>
                    
                    method <span class="op">=</span> <span class="st">"skipgram"</span>,                   <span class="co"># method = 'skipgram'  </span>
                    
                    thread <span class="op">=</span> <span class="fl">4</span>, verbose <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>Read 124M words</span>
<span id="cb15-2"><a href="#cb15-2"></a>Number of words<span class="op">:</span><span class="st">  </span><span class="dv">5029370</span></span>
<span id="cb15-3"><a href="#cb15-3"></a>Number of labels<span class="op">:</span><span class="st"> </span><span class="dv">0</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>Progress<span class="op">:</span><span class="st"> </span><span class="fl">100.0</span>%  words<span class="op">/</span>sec<span class="op">/</span>thread<span class="op">:</span><span class="st"> </span><span class="dv">94027</span>  lr<span class="op">:</span><span class="st"> </span><span class="fl">0.000000</span>  loss<span class="op">:</span><span class="st"> </span><span class="fl">0.186674</span>  eta<span class="op">:</span><span class="st"> </span>0h0m </span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a>time to complete <span class="op">:</span><span class="st"> </span><span class="fl">33.53577</span> mins </span></code></pre></div>
<p><br></p>
<p>the following vector-subset is the example output of the “model.vec” file, which includes the 50-dimensional word vectors,</p>
<p><br></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>lt <span class="fl">0.12207</span> <span class="fl">0.16117</span> <span class="fl">0.4641</span> <span class="fl">0.73876</span> <span class="fl">0.43968</span> <span class="fl">0.63911</span> <span class="fl">-0.53747</span> <span class="fl">0.1398</span> ..... </span>
<span id="cb16-2"><a href="#cb16-2"></a>refgt <span class="fl">-0.0038898</span> <span class="fl">-0.13976</span> <span class="fl">0.26077</span> <span class="fl">0.7775</span> <span class="fl">0.2228</span> <span class="fl">0.28169</span> <span class="fl">-0.48306</span> .....</span>
<span id="cb16-3"><a href="#cb16-3"></a>quot <span class="fl">0.7295</span> <span class="fl">-0.12472</span> <span class="fl">0.32131</span> <span class="fl">0.46965</span> <span class="fl">0.45363</span> <span class="fl">0.85022</span> <span class="fl">-0.051471</span> ..... </span>
<span id="cb16-4"><a href="#cb16-4"></a>gt <span class="fl">0.41287</span> <span class="fl">0.26584</span> <span class="fl">0.6612</span> <span class="fl">0.78185</span> <span class="fl">0.46692</span> <span class="fl">0.74092</span> <span class="fl">-0.23816</span> .....</span>
<span id="cb16-5"><a href="#cb16-5"></a>cite <span class="fl">0.037943</span> <span class="fl">0.095684</span> <span class="fl">0.62832</span> <span class="fl">0.93794</span> <span class="fl">0.19776</span> <span class="fl">0.44592</span> <span class="fl">-0.21641</span> .....</span>
<span id="cb16-6"><a href="#cb16-6"></a>www <span class="fl">-0.31855</span> <span class="fl">0.42268</span> <span class="fl">0.3875</span> <span class="fl">1.5457</span> <span class="fl">-0.23804</span> <span class="fl">0.34022</span> <span class="fl">-0.051849</span> ..... </span>
<span id="cb16-7"><a href="#cb16-7"></a>ref <span class="fl">0.45236</span> <span class="fl">-0.21766</span> <span class="fl">0.6341</span> <span class="fl">0.76392</span> <span class="fl">0.53734</span> <span class="fl">0.66976</span> <span class="fl">-0.23162</span> .....</span>
<span id="cb16-8"><a href="#cb16-8"></a>http <span class="fl">-0.42692</span> <span class="fl">0.48637</span> <span class="fl">0.28622</span> <span class="fl">1.7019</span> <span class="fl">-0.25739</span> <span class="fl">0.25948</span> <span class="fl">-0.026582</span> ..... </span>
<span id="cb16-9"><a href="#cb16-9"></a>namequot <span class="fl">0.56828</span> <span class="fl">-0.30782</span> <span class="fl">0.45707</span> <span class="fl">0.78346</span> <span class="fl">0.53727</span> <span class="fl">0.62445</span> ..... </span>
<span id="cb16-10"><a href="#cb16-10"></a><span class="op">-</span><span class="st"> </span><span class="fl">-0.010281</span> <span class="fl">0.25528</span> <span class="fl">0.04708</span> <span class="fl">0.49679</span> <span class="fl">0.043934</span> <span class="fl">0.33733</span> <span class="fl">-0.42706</span> .....</span>
<span id="cb16-11"><a href="#cb16-11"></a>amp <span class="fl">0.06308</span> <span class="fl">0.11968</span> <span class="fl">0.11885</span> <span class="fl">0.67699</span> <span class="fl">-0.11448</span> <span class="fl">0.25183</span> <span class="fl">-0.48789</span> .....</span>
<span id="cb16-12"><a href="#cb16-12"></a>category <span class="fl">-1.5705</span> <span class="fl">-0.40638</span> <span class="fl">0.61064</span> <span class="fl">2.5691</span> <span class="fl">-0.52987</span> <span class="fl">0.68096</span> .....</span>
<span id="cb16-13"><a href="#cb16-13"></a>county <span class="fl">-0.85743</span> <span class="fl">0.071625</span> <span class="fl">-0.43393</span> <span class="fl">0.17157</span> <span class="fl">-0.32874</span> <span class="fl">1.771</span> ..... </span>
<span id="cb16-14"><a href="#cb16-14"></a>org <span class="fl">-0.26974</span> <span class="fl">0.76983</span> <span class="fl">0.57599</span> <span class="fl">1.5939</span> <span class="fl">-0.1706</span> <span class="fl">0.21937</span> <span class="fl">-0.44645</span> .....</span>
<span id="cb16-15"><a href="#cb16-15"></a>states <span class="fl">-0.40973</span> <span class="fl">-0.48528</span> <span class="fl">0.092905</span> <span class="fl">0.011603</span> <span class="fl">-0.035727</span> <span class="fl">0.52807</span> .....</span>
<span id="cb16-16"><a href="#cb16-16"></a>united <span class="fl">-0.25079</span> <span class="fl">-0.49813</span> <span class="fl">0.070942</span> <span class="fl">0.16762</span> <span class="fl">0.069961</span> <span class="fl">0.56868</span> .....</span>
<span id="cb16-17"><a href="#cb16-17"></a>web <span class="fl">-0.066578</span> <span class="fl">0.14837</span> <span class="fl">0.23088</span> <span class="fl">1.2919</span> <span class="fl">-0.252</span> <span class="fl">0.31441</span> <span class="fl">-0.3799</span> ..... </span>
<span id="cb16-18"><a href="#cb16-18"></a>census <span class="fl">-0.29033</span> <span class="fl">-0.73695</span> <span class="fl">0.35474</span> <span class="fl">-0.5237</span> <span class="fl">-0.15206</span> <span class="fl">1.7089</span> .....</span>
<span id="cb16-19"><a href="#cb16-19"></a>.</span>
<span id="cb16-20"><a href="#cb16-20"></a>.</span>
<span id="cb16-21"><a href="#cb16-21"></a>.</span></code></pre></div>
<p><br></p>
</div>
</div>
<div id="sparse_term_matrix-class" class="section level2">
<h2 class="hasAnchor">
<a href="#sparse_term_matrix-class" class="anchor"></a><em>sparse_term_matrix</em> class</h2>
<p>The <em>sparse_term_matrix</em> class includes methods for building a document-term or a term-document matrix and extracting information from those matrices (it relies on RcppArmadillo and can handle large sparse matrices too). I’ll explain all the different methods using a toy text file downloaded from wikipedia,</p>
<p><br></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>The term planet is ancient, with ties to history, astrology, science, mythology, and religion. Several planets <span class="cf">in</span> the Solar System can be seen with the naked eye. These were regarded by many early cultures as divine, or as emissaries of deities. As scientific knowledge advanced, human perception of the planets changed, incorporating a number of disparate objects. In <span class="dv">2006</span>, the International Astronomical <span class="kw">Union</span> (IAU) officially adopted a resolution defining planets within the Solar System. This definition is controversial because it excludes many objects of planetary mass based on where or what they orbit. </span>
<span id="cb17-2"><a href="#cb17-2"></a>Although eight of the planetary bodies discovered before <span class="dv">1950</span> remain planets under the modern definition, some celestial bodies, such as Ceres, Pallas, Juno and <span class="kw">Vesta</span> (each an object <span class="cf">in</span> the solar asteroid belt), and <span class="kw">Pluto</span> (the first trans<span class="op">-</span>Neptunian object discovered), that were once considered planets by the scientific community, are no longer viewed as such.</span>
<span id="cb17-3"><a href="#cb17-3"></a>The planets were thought by Ptolemy to orbit Earth <span class="cf">in</span> deferent and epicycle motions. Although the idea that the planets orbited the Sun had been suggested many times, it was not until the 17th century that this view was supported by evidence from the first telescopic astronomical observations, performed by Galileo Galilei. </span>
<span id="cb17-4"><a href="#cb17-4"></a>At about the same time, by careful analysis of pre<span class="op">-</span>telescopic observation data collected by Tycho Brahe, Johannes Kepler found the planets orbits were not circular but elliptical. As observational tools improved, astronomers saw that, like Earth, the planets rotated around tilted axes, and some shared such features as ice caps and seasons. Since the dawn of the Space Age, close observation by space probes has found that Earth and the other planets share characteristics such as volcanism, hurricanes, tectonics, and even hydrology.</span>
<span id="cb17-5"><a href="#cb17-5"></a>Planets are generally divided into two main types<span class="op">:</span><span class="st"> </span>large low<span class="op">-</span>density giant planets, and smaller rocky terrestrials. Under IAU definitions, there are eight planets <span class="cf">in</span> the Solar System. In order of increasing distance from the Sun, they are the four terrestrials, Mercury, Venus, Earth, and Mars, then the four giant planets, Jupiter, Saturn, Uranus, and Neptune. Six of the planets are orbited by one or more natural satellites.</span></code></pre></div>
<p><br></p>
<p>The <em>sparse_term_matrix</em> class can be initialized using either a vector of documents or a text file. Assuming the downloaded file is saved as “planets.txt”, then a document-term-matrix can be created in the following way,</p>
<p><br></p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/sparse_term_matrix.html">sparse_term_matrix</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>vector_data <span class="op">=</span> <span class="cn">NULL</span>,          <span class="co"># in case of vector of documents</span>
                              
                              file_data <span class="op">=</span> <span class="st">"/planets.txt"</span>,     <span class="co"># input the .txt data</span>
                              
                              document_term_matrix <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>   <span class="co"># document term matrix as output</span>



<span class="va">tm</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">Term_Matrix</span><span class="op">(</span>sort_terms <span class="op">=</span> <span class="cn">TRUE</span>,      <span class="co"># initial terms are sorted</span>
                 
                      to_lower <span class="op">=</span> <span class="cn">TRUE</span>,          <span class="co"># convert to lower case</span>
                 
                      trim_token <span class="op">=</span> <span class="cn">TRUE</span>,        <span class="co"># trim token</span>
                 
                      split_string <span class="op">=</span> <span class="cn">TRUE</span>,      <span class="co"># split string</span>
                 
                      tf_idf <span class="op">=</span> <span class="cn">TRUE</span>,            <span class="co"># tf-idf will be returned</span>
                      
                      verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>minutes.to.tokenize.transform.data<span class="op">:</span><span class="st"> </span><span class="fl">0.00001</span> total.time<span class="op">:</span><span class="st"> </span><span class="fl">0.00001</span></span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a>Warning message<span class="op">:</span></span>
<span id="cb19-4"><a href="#cb19-4"></a>empty character strings present <span class="cf">in</span> the column names they will be replaced with proper characters</span>
<span id="cb19-5"><a href="#cb19-5"></a></span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="dv">5</span> x <span class="dv">212</span> sparse Matrix of class <span class="st">"dgCMatrix"</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>   [[ suppressing <span class="dv">91</span> column names <span class="st">'X'</span>, <span class="st">'X17th'</span>, <span class="st">'X1950'</span> ... ]]</span>
<span id="cb19-8"><a href="#cb19-8"></a></span>
<span id="cb19-9"><a href="#cb19-9"></a>[<span class="dv">1</span>,] <span class="fl">-0.001939591</span> .         .          <span class="fl">0.009747774</span> <span class="fl">0.01949555</span>   ......         </span>
<span id="cb19-10"><a href="#cb19-10"></a>[<span class="dv">2</span>,] <span class="fl">-0.003255742</span> .         <span class="fl">0.01636233</span> .           .            ......         </span>
<span id="cb19-11"><a href="#cb19-11"></a>[<span class="dv">3</span>,] <span class="fl">-0.003440029</span> <span class="fl">0.0172885</span> .          .           .            ......         </span>
<span id="cb19-12"><a href="#cb19-12"></a>[<span class="dv">4</span>,] <span class="fl">-0.002196645</span> .         .          .           .            ......  </span>
<span id="cb19-13"><a href="#cb19-13"></a>[<span class="dv">5</span>,] <span class="fl">-0.002681199</span> .         .          .           .          . ......</span>
<span id="cb19-14"><a href="#cb19-14"></a></span>
<span id="cb19-15"><a href="#cb19-15"></a>[<span class="dv">1</span>,] <span class="fl">0.007121603</span> .          <span class="fl">0.009747774</span> .          <span class="fl">0.005434315</span> . ......        </span>
<span id="cb19-16"><a href="#cb19-16"></a>[<span class="dv">2</span>,] <span class="fl">0.007969413</span> <span class="fl">0.01636233</span> .           .          .           . ......    </span>
<span id="cb19-17"><a href="#cb19-17"></a>[<span class="dv">3</span>,] .           .          .           .          <span class="fl">0.009638219</span> . ......  </span>
<span id="cb19-18"><a href="#cb19-18"></a>[<span class="dv">4</span>,] <span class="fl">0.008065430</span> .          .           <span class="fl">0.01103965</span> .             ......  </span>
<span id="cb19-19"><a href="#cb19-19"></a>[<span class="dv">5</span>,] .           .          .           .          .             ......</span>
<span id="cb19-20"><a href="#cb19-20"></a></span>
<span id="cb19-21"><a href="#cb19-21"></a>[<span class="dv">1</span>,] <span class="fl">-0.001939591</span> <span class="fl">0.009747774</span> .          .          .           ......  </span>
<span id="cb19-22"><a href="#cb19-22"></a>[<span class="dv">2</span>,] <span class="fl">-0.003255742</span> .           .          .          <span class="fl">0.01636233</span>  ......          </span>
<span id="cb19-23"><a href="#cb19-23"></a>[<span class="dv">3</span>,] <span class="fl">-0.010320088</span> .           .          .          .           ......          </span>
<span id="cb19-24"><a href="#cb19-24"></a>[<span class="dv">4</span>,] <span class="fl">-0.006589936</span> .           <span class="fl">0.01103965</span> <span class="fl">0.01103965</span> .           ......        </span>
<span id="cb19-25"><a href="#cb19-25"></a>[<span class="dv">5</span>,] <span class="fl">-0.002681199</span> .           .          .          .           ......          </span>
<span id="cb19-26"><a href="#cb19-26"></a>.</span>
<span id="cb19-27"><a href="#cb19-27"></a>.</span>
<span id="cb19-28"><a href="#cb19-28"></a>.</span></code></pre></div>
<p><br></p>
<p>The <em>Term_Matrix</em> method takes almost the same parameters as the ( already explained ) <em>big_text_tokenizer()</em> method. The only differences are:</p>
<ul>
<li>
<strong>sort_terms</strong> ( should the output terms - rows or columns depending on the <em>document_term_matrix</em> parameter - be sorted in alphabetical order )</li>
<li>
<strong>print_every_rows</strong> ( verbose output intervalls )</li>
<li>
<strong>normalize</strong> ( applies <em>l1</em> or <em>l2</em> normalization )</li>
<li>
<strong>tf_idf</strong> ( the term-frequency-inverse-document-frequency will be returned )</li>
</ul>
<p>Details about the parameters of the <em>Term_Matrix</em> method can be found in the package documentation. <br></p>
<p><br></p>
<p>To adjust the sparsity of the output matrix one can take advantage of the <em>Term_Matrix_Adjust</em> method, (by adjusting the sparsity_thresh towards 0.0 a proportion of the sparse terms will be removed)</p>
<p><br></p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res_adj</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">Term_Matrix_Adjust</span><span class="op">(</span>sparsity_thresh <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span>    </code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>res_adj</span>
<span id="cb21-2"><a href="#cb21-2"></a></span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="dv">5</span> x <span class="dv">9</span> sparse Matrix of class <span class="st">"dgCMatrix"</span></span>
<span id="cb21-5"><a href="#cb21-5"></a>          planets           by            X       solar          and          as  ...... </span>
<span id="cb21-6"><a href="#cb21-6"></a>[<span class="dv">1</span>,] <span class="fl">-0.005818773</span> <span class="fl">-0.001939591</span> <span class="fl">-0.001939591</span> <span class="fl">0.004747735</span> <span class="fl">-0.001939591</span> <span class="fl">0.007121603</span>  ......      </span>
<span id="cb21-7"><a href="#cb21-7"></a>[<span class="dv">2</span>,] <span class="fl">-0.006511484</span> <span class="fl">-0.003255742</span> <span class="fl">-0.003255742</span> <span class="fl">0.003984706</span> <span class="fl">-0.006511484</span> <span class="fl">0.007969413</span>  ......       </span>
<span id="cb21-8"><a href="#cb21-8"></a>[<span class="dv">3</span>,] <span class="fl">-0.006880059</span> <span class="fl">-0.010320088</span> <span class="fl">-0.003440029</span> .           <span class="fl">-0.003440029</span> .            ...... </span>
<span id="cb21-9"><a href="#cb21-9"></a>[<span class="dv">4</span>,] <span class="fl">-0.006589936</span> <span class="fl">-0.006589936</span> <span class="fl">-0.002196645</span> .           <span class="fl">-0.008786581</span> <span class="fl">0.008065430</span>  ...... </span>
<span id="cb21-10"><a href="#cb21-10"></a>[<span class="dv">5</span>,] <span class="fl">-0.013405997</span> <span class="fl">-0.002681199</span> <span class="fl">-0.002681199</span> <span class="fl">0.003281523</span> <span class="fl">-0.008043598</span> .            ...... </span></code></pre></div>
<p><br></p>
<p>The <em>term_associations</em> method returns the correlation of specific terms (<em>Terms</em>) with all the other terms in the output matrix. The dimensions of the output matrix can vary depending on which one of the <em>Term_Matrix</em>, <em>Term_Matrix_Adjust</em> is run previously. In the previous step I adjusted the initial sparse matrix using a sparsity_thresh of 0.6, thus the new dimensions will be,</p>
<p><br></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="kw">dim</span>(res_adj)</span>
<span id="cb22-2"><a href="#cb22-2"></a></span>
<span id="cb22-3"><a href="#cb22-3"></a>[<span class="dv">1</span>] <span class="dv">5</span> <span class="dv">9</span></span></code></pre></div>
<p><br></p>
<p>and the resulted terms,</p>
<p><br></p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span><span class="op">$</span><span class="fu">term_associations</span><span class="op">(</span>Terms <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'planets'</span>, <span class="st">'by'</span>, <span class="st">'INVALID'</span><span class="op">)</span>, keep_terms <span class="op">=</span> <span class="cn">NULL</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>the <span class="st">' INVALID '</span> term does not exist <span class="cf">in</span> the terms vector </span>
<span id="cb24-2"><a href="#cb24-2"></a></span>
<span id="cb24-3"><a href="#cb24-3"></a>total.number.variables.processed<span class="op">:</span><span class="st">   </span><span class="dv">2</span>   minutes.to.complete<span class="op">:</span><span class="st"> </span><span class="fl">0.00000</span></span>
<span id="cb24-4"><a href="#cb24-4"></a></span>
<span id="cb24-5"><a href="#cb24-5"></a><span class="op">$</span>planets</span>
<span id="cb24-6"><a href="#cb24-6"></a>     term correlation</span>
<span id="cb24-7"><a href="#cb24-7"></a> <span class="dv">1</span><span class="op">:</span><span class="st">    </span>as  <span class="fl">0.65943196</span></span>
<span id="cb24-8"><a href="#cb24-8"></a> <span class="dv">2</span><span class="op">:</span><span class="st">   </span>and  <span class="fl">0.48252671</span></span>
<span id="cb24-9"><a href="#cb24-9"></a> <span class="dv">3</span><span class="op">:</span><span class="st">     </span>X  <span class="fl">0.07521813</span></span>
<span id="cb24-10"><a href="#cb24-10"></a> <span class="dv">4</span><span class="op">:</span><span class="st">    </span>by <span class="fl">-0.26301349</span></span>
<span id="cb24-11"><a href="#cb24-11"></a> <span class="dv">5</span><span class="op">:</span><span class="st">    </span>of         <span class="ot">NaN</span></span>
<span id="cb24-12"><a href="#cb24-12"></a> <span class="dv">6</span><span class="op">:</span><span class="st"> </span>solar <span class="fl">-0.11887462</span></span>
<span id="cb24-13"><a href="#cb24-13"></a> <span class="dv">7</span><span class="op">:</span><span class="st">  </span>were         <span class="ot">NaN</span></span>
<span id="cb24-14"><a href="#cb24-14"></a> <span class="dv">8</span><span class="op">:</span><span class="st">   </span>the <span class="fl">-0.15500900</span></span>
<span id="cb24-15"><a href="#cb24-15"></a> <span class="dv">9</span><span class="op">:</span><span class="st">   </span>in.         <span class="ot">NaN</span></span>
<span id="cb24-16"><a href="#cb24-16"></a><span class="dv">10</span><span class="op">:</span><span class="st">  </span>that  <span class="fl">0.44307617</span></span>
<span id="cb24-17"><a href="#cb24-17"></a><span class="dv">11</span><span class="op">:</span><span class="st"> </span>earth <span class="fl">-0.24226093</span></span>
<span id="cb24-18"><a href="#cb24-18"></a></span>
<span id="cb24-19"><a href="#cb24-19"></a><span class="op">$</span>by</span>
<span id="cb24-20"><a href="#cb24-20"></a>       term correlation</span>
<span id="cb24-21"><a href="#cb24-21"></a> <span class="dv">1</span><span class="op">:</span><span class="st">   </span>solar   <span class="fl">0.9092777</span></span>
<span id="cb24-22"><a href="#cb24-22"></a> <span class="dv">2</span><span class="op">:</span><span class="st">       </span>X   <span class="fl">0.5010034</span></span>
<span id="cb24-23"><a href="#cb24-23"></a> <span class="dv">3</span><span class="op">:</span><span class="st"> </span>planets  <span class="fl">-0.2630135</span></span>
<span id="cb24-24"><a href="#cb24-24"></a> <span class="dv">4</span><span class="op">:</span><span class="st">      </span>of         <span class="ot">NaN</span></span>
<span id="cb24-25"><a href="#cb24-25"></a> <span class="dv">5</span><span class="op">:</span><span class="st">    </span>were         <span class="ot">NaN</span></span>
<span id="cb24-26"><a href="#cb24-26"></a> <span class="dv">6</span><span class="op">:</span><span class="st">     </span>the   <span class="fl">0.7838436</span></span>
<span id="cb24-27"><a href="#cb24-27"></a> <span class="dv">7</span><span class="op">:</span><span class="st">      </span>as   <span class="fl">0.3698239</span></span>
<span id="cb24-28"><a href="#cb24-28"></a> <span class="dv">8</span><span class="op">:</span><span class="st">     </span>and  <span class="fl">-0.0594149</span></span>
<span id="cb24-29"><a href="#cb24-29"></a> <span class="dv">9</span><span class="op">:</span><span class="st">     </span>in.         <span class="ot">NaN</span></span>
<span id="cb24-30"><a href="#cb24-30"></a><span class="dv">10</span><span class="op">:</span><span class="st">   </span>earth  <span class="fl">-0.6952757</span></span>
<span id="cb24-31"><a href="#cb24-31"></a><span class="dv">11</span><span class="op">:</span><span class="st">    </span>that  <span class="fl">-0.9338884</span></span></code></pre></div>
<p><br></p>
<p>Lastly, the <em>most_frequent_terms</em> method gives the frequency of the terms in the corpus. However, the function returns only if the <em>normalize</em> parameter is NULL and the <em>tf_idf</em> parameter is FALSE ( the latter two parameters belong to the <em>init$Term_Matrix()</em> method ),</p>
<p><br></p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/sparse_term_matrix.html">sparse_term_matrix</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>file_data <span class="op">=</span> <span class="va">PATH</span>, document_term_matrix <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="va">tm</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">Term_Matrix</span><span class="op">(</span>sort_terms <span class="op">=</span> <span class="cn">TRUE</span>,     
                 
                      to_lower <span class="op">=</span> <span class="cn">TRUE</span>,          
                 
                      trim_token <span class="op">=</span> <span class="cn">TRUE</span>,       
                 
                      split_string <span class="op">=</span> <span class="cn">TRUE</span>,     
                 
                      tf_idf <span class="op">=</span> <span class="cn">FALSE</span>,            <span class="co"># disable tf-idf</span>
                      
                      verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>


<span class="va">init</span><span class="op">$</span><span class="fu">most_frequent_terms</span><span class="op">(</span>keep_terms <span class="op">=</span> <span class="fl">10</span>, threads <span class="op">=</span> <span class="fl">1</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>minutes.to.complete<span class="op">:</span><span class="st"> </span><span class="fl">0.00000</span></span>
<span id="cb26-2"><a href="#cb26-2"></a></span>
<span id="cb26-3"><a href="#cb26-3"></a>        term frequency</span>
<span id="cb26-4"><a href="#cb26-4"></a> <span class="dv">1</span><span class="op">:</span><span class="st">     </span>the        <span class="dv">28</span></span>
<span id="cb26-5"><a href="#cb26-5"></a> <span class="dv">2</span><span class="op">:</span><span class="st"> </span>planets        <span class="dv">15</span></span>
<span id="cb26-6"><a href="#cb26-6"></a> <span class="dv">3</span><span class="op">:</span><span class="st">     </span>and        <span class="dv">11</span></span>
<span id="cb26-7"><a href="#cb26-7"></a> <span class="dv">4</span><span class="op">:</span><span class="st">      </span>of         <span class="dv">9</span></span>
<span id="cb26-8"><a href="#cb26-8"></a> <span class="dv">5</span><span class="op">:</span><span class="st">      </span>by         <span class="dv">9</span></span>
<span id="cb26-9"><a href="#cb26-9"></a> <span class="dv">6</span><span class="op">:</span><span class="st">      </span>as         <span class="dv">8</span></span>
<span id="cb26-10"><a href="#cb26-10"></a> <span class="dv">7</span><span class="op">:</span><span class="st">     </span>in.         <span class="dv">6</span></span>
<span id="cb26-11"><a href="#cb26-11"></a> <span class="dv">8</span><span class="op">:</span><span class="st">       </span>X         <span class="dv">5</span></span>
<span id="cb26-12"><a href="#cb26-12"></a> <span class="dv">9</span><span class="op">:</span><span class="st">     </span>are         <span class="dv">5</span></span>
<span id="cb26-13"><a href="#cb26-13"></a><span class="dv">10</span><span class="op">:</span><span class="st">    </span>that         <span class="dv">5</span></span></code></pre></div>
<p><br></p>
<p>More information about the <em>sparse_term_matrix</em> class can be found in the package documentation.</p>
<p><br></p>
</div>
<div id="token_stats-class" class="section level2">
<h2 class="hasAnchor">
<a href="#token_stats-class" class="anchor"></a><em>token_stats</em> class</h2>
<p>The <em>token_stats</em> class can be utilized to output corpus statistics. Each of the following methods can take either a <em>vector of terms</em>, a <em>text file</em> or <em>a folder of files</em> as input:</p>
<p><br></p>
<ul>
<li>
<strong>path_2vector</strong> : is a helper method which takes a path to a file or folder of files and returns the content in form of a vector, <br>
</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">"/planets.txt"</span><span class="op">)</span>    <span class="co"># input the 'planets.txt' file</span>

<span class="va">vec</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">path_2vector</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a>[<span class="dv">1</span>] <span class="st">"The term planet is ancient, with ties to history, astrology, science, mythology, and religion. Several planets in the Solar System can be seen with the naked eye. These were regarded by many early cultures as divine, or as emissaries of deities. As scientific knowledge advanced, human perception of the planets changed, incorporating a number of disparate objects"</span> ....</span>
<span id="cb28-2"><a href="#cb28-2"></a></span>
<span id="cb28-3"><a href="#cb28-3"></a>[<span class="dv">2</span>] <span class="st">"Although eight of the planetary bodies discovered before 1950 remain"</span> ....</span>
<span id="cb28-4"><a href="#cb28-4"></a>.</span>
<span id="cb28-5"><a href="#cb28-5"></a>.</span></code></pre></div>
<p><br></p>
<ul>
<li>
<strong>freq_distribution</strong> : it returns a named-unsorted vector frequency distribution for a vocabulary file</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="co"># assuming the following 'freq_vocab.txt'</span></span>
<span id="cb29-2"><a href="#cb29-2"></a></span>
<span id="cb29-3"><a href="#cb29-3"></a>the</span>
<span id="cb29-4"><a href="#cb29-4"></a>term</span>
<span id="cb29-5"><a href="#cb29-5"></a>planet</span>
<span id="cb29-6"><a href="#cb29-6"></a>is</span>
<span id="cb29-7"><a href="#cb29-7"></a>ancient</span>
<span id="cb29-8"><a href="#cb29-8"></a>with</span>
<span id="cb29-9"><a href="#cb29-9"></a>ties</span>
<span id="cb29-10"><a href="#cb29-10"></a>to</span>
<span id="cb29-11"><a href="#cb29-11"></a>history</span>
<span id="cb29-12"><a href="#cb29-12"></a>astrology</span>
<span id="cb29-13"><a href="#cb29-13"></a>science</span>
<span id="cb29-14"><a href="#cb29-14"></a>mythology</span>
<span id="cb29-15"><a href="#cb29-15"></a>and</span>
<span id="cb29-16"><a href="#cb29-16"></a>religion</span>
<span id="cb29-17"><a href="#cb29-17"></a>several</span>
<span id="cb29-18"><a href="#cb29-18"></a>planets</span>
<span id="cb29-19"><a href="#cb29-19"></a><span class="cf">in</span></span>
<span id="cb29-20"><a href="#cb29-20"></a>the</span>
<span id="cb29-21"><a href="#cb29-21"></a>solar</span>
<span id="cb29-22"><a href="#cb29-22"></a>system</span>
<span id="cb29-23"><a href="#cb29-23"></a>can</span>
<span id="cb29-24"><a href="#cb29-24"></a>be</span>
<span id="cb29-25"><a href="#cb29-25"></a>seen</span>
<span id="cb29-26"><a href="#cb29-26"></a>with</span>
<span id="cb29-27"><a href="#cb29-27"></a>the</span>
<span id="cb29-28"><a href="#cb29-28"></a>naked</span>
<span id="cb29-29"><a href="#cb29-29"></a>eye</span>
<span id="cb29-30"><a href="#cb29-30"></a>these</span>
<span id="cb29-31"><a href="#cb29-31"></a>were</span></code></pre></div>
<p><br></p>
<p>this method would return,</p>
<p><br></p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">'freq_vocab.txt'</span><span class="op">)</span>

<span class="va">init</span><span class="op">$</span><span class="fu">freq_distribution</span><span class="op">(</span><span class="op">)</span>

<span class="va">init</span><span class="op">$</span><span class="fu">print_frequency</span><span class="op">(</span>subset <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a>        words freq</span>
<span id="cb31-2"><a href="#cb31-2"></a> <span class="dv">1</span><span class="op">:</span><span class="st">       </span>the    <span class="dv">3</span></span>
<span id="cb31-3"><a href="#cb31-3"></a> <span class="dv">2</span><span class="op">:</span><span class="st">      </span>with    <span class="dv">2</span></span>
<span id="cb31-4"><a href="#cb31-4"></a> <span class="dv">3</span><span class="op">:</span><span class="st">   </span>ancient    <span class="dv">1</span></span>
<span id="cb31-5"><a href="#cb31-5"></a> <span class="dv">4</span><span class="op">:</span><span class="st">       </span>and    <span class="dv">1</span></span>
<span id="cb31-6"><a href="#cb31-6"></a> <span class="dv">5</span><span class="op">:</span><span class="st"> </span>astrology    <span class="dv">1</span></span>
<span id="cb31-7"><a href="#cb31-7"></a> <span class="dv">6</span><span class="op">:</span><span class="st">        </span>be    <span class="dv">1</span></span>
<span id="cb31-8"><a href="#cb31-8"></a> <span class="dv">7</span><span class="op">:</span><span class="st">       </span>can    <span class="dv">1</span></span>
<span id="cb31-9"><a href="#cb31-9"></a> <span class="dv">8</span><span class="op">:</span><span class="st">       </span>eye    <span class="dv">1</span></span>
<span id="cb31-10"><a href="#cb31-10"></a> <span class="dv">9</span><span class="op">:</span><span class="st">   </span>history    <span class="dv">1</span></span>
<span id="cb31-11"><a href="#cb31-11"></a><span class="dv">10</span><span class="op">:</span><span class="st">        </span><span class="cf">in</span>    <span class="dv">1</span></span>
<span id="cb31-12"><a href="#cb31-12"></a><span class="dv">11</span><span class="op">:</span><span class="st">        </span>is    <span class="dv">1</span></span>
<span id="cb31-13"><a href="#cb31-13"></a><span class="dv">12</span><span class="op">:</span><span class="st"> </span>mythology    <span class="dv">1</span></span>
<span id="cb31-14"><a href="#cb31-14"></a><span class="dv">13</span><span class="op">:</span><span class="st">     </span>naked    <span class="dv">1</span></span>
<span id="cb31-15"><a href="#cb31-15"></a><span class="dv">14</span><span class="op">:</span><span class="st">    </span>planet    <span class="dv">1</span></span>
<span id="cb31-16"><a href="#cb31-16"></a><span class="dv">15</span><span class="op">:</span><span class="st">   </span>planets    <span class="dv">1</span></span>
<span id="cb31-17"><a href="#cb31-17"></a><span class="dv">16</span><span class="op">:</span><span class="st">  </span>religion    <span class="dv">1</span></span>
<span id="cb31-18"><a href="#cb31-18"></a><span class="dv">17</span><span class="op">:</span><span class="st">   </span>science    <span class="dv">1</span></span>
<span id="cb31-19"><a href="#cb31-19"></a><span class="dv">18</span><span class="op">:</span><span class="st">      </span>seen    <span class="dv">1</span></span>
<span id="cb31-20"><a href="#cb31-20"></a><span class="dv">19</span><span class="op">:</span><span class="st">   </span>several    <span class="dv">1</span></span>
<span id="cb31-21"><a href="#cb31-21"></a><span class="dv">20</span><span class="op">:</span><span class="st">     </span>solar    <span class="dv">1</span></span>
<span id="cb31-22"><a href="#cb31-22"></a><span class="dv">21</span><span class="op">:</span><span class="st">    </span>system    <span class="dv">1</span></span>
<span id="cb31-23"><a href="#cb31-23"></a><span class="dv">22</span><span class="op">:</span><span class="st">      </span>term    <span class="dv">1</span></span>
<span id="cb31-24"><a href="#cb31-24"></a><span class="dv">23</span><span class="op">:</span><span class="st">     </span>these    <span class="dv">1</span></span>
<span id="cb31-25"><a href="#cb31-25"></a><span class="dv">24</span><span class="op">:</span><span class="st">      </span>ties    <span class="dv">1</span></span>
<span id="cb31-26"><a href="#cb31-26"></a><span class="dv">25</span><span class="op">:</span><span class="st">        </span>to    <span class="dv">1</span></span>
<span id="cb31-27"><a href="#cb31-27"></a><span class="dv">26</span><span class="op">:</span><span class="st">      </span>were    <span class="dv">1</span></span>
<span id="cb31-28"><a href="#cb31-28"></a>        words freq</span></code></pre></div>
<p><br></p>
<ul>
<li>
<strong>count_character</strong> : it returns the number of characters for each word of the corpus.</li>
</ul>
<p><br></p>
<p>for the previously mentioned <em>‘freq_vocab.txt’</em> it would output,</p>
<p><br></p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R">  <span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">'freq_vocab.txt'</span><span class="op">)</span>

  <span class="va">vec_tmp</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">count_character</span><span class="op">(</span><span class="op">)</span>
  
  <span class="va">init</span><span class="op">$</span><span class="fu">print_count_character</span><span class="op">(</span>number <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># words with number of characters equal to 3</span></span>
<span id="cb33-2"><a href="#cb33-2"></a></span>
<span id="cb33-3"><a href="#cb33-3"></a>[<span class="dv">1</span>] <span class="st">"the"</span> <span class="st">"and"</span> <span class="st">"the"</span> <span class="st">"can"</span> <span class="st">"the"</span> <span class="st">"eye"</span></span></code></pre></div>
<p><br></p>
<ul>
<li>
<strong>collocation_words</strong> : it returns a co-occurence frequency table for n-grams. “A <a href="http://nlp.stanford.edu/fsnlp/promo/colloc.pdf">collocation</a> is defined as a sequence of two or more consecutive words, that has characteristics of a syntactic and semantic unit, and whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components”. The input to the function should be text n-grams separated by a delimiter ( for instance the <em>tokenize_transform_text()</em> function in the next code chunk will build n_grams of length 3 ),</li>
</ul>
<p><br></p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># the data needs to be n-grams thus first tokenize and build the n-grams using </span>
<span class="co"># the 'tokenize_transform_text' function ( the "planets.txt" file as input )</span>

<span class="va">tok</span> <span class="op">=</span> <span class="fu"><a href="../reference/tokenize_transform_text.html">tokenize_transform_text</a></span><span class="op">(</span><span class="st">"planets.txt"</span>,  
                              
                              to_lower <span class="op">=</span> <span class="cn">T</span>, 
                              
                              split_string <span class="op">=</span> <span class="cn">T</span>,
                              
                              min_n_gram <span class="op">=</span> <span class="fl">3</span>, 
                              
                              max_n_gram <span class="op">=</span> <span class="fl">3</span>, 
                              
                              n_gram_delimiter <span class="op">=</span> <span class="st">"_"</span><span class="op">)</span>

<span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>x_vec <span class="op">=</span> <span class="va">tok</span><span class="op">$</span><span class="va">token</span><span class="op">)</span>      <span class="co"># vector data as input</span>

<span class="va">vec_tmp</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">collocation_words</span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>the example output of the <em>vec_tmp</em> vector is,</p>
<p><br></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>[<span class="dv">1</span>] <span class="st">""</span>       <span class="st">"17th"</span>       <span class="st">"1950"</span>    <span class="st">"2006"</span>    <span class="st">"a"</span>     <span class="st">"about"</span>   <span class="st">"adopted"</span>   <span class="st">"advanced"</span>       </span>
<span id="cb35-2"><a href="#cb35-2"></a>[<span class="dv">9</span>] <span class="st">"age"</span>    <span class="st">"although"</span> ....  </span>
<span id="cb35-3"><a href="#cb35-3"></a>.</span>
<span id="cb35-4"><a href="#cb35-4"></a>.</span>
<span id="cb35-5"><a href="#cb35-5"></a>.</span></code></pre></div>
<p><br></p>
<p>and the <em>print_collocations</em> method returns the coolocations for the example word <em>ancient</em>,</p>
<p><br></p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">print_collocations</span><span class="op">(</span>word <span class="op">=</span> <span class="st">"ancient"</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a>    is   with   ties planet </span>
<span id="cb37-2"><a href="#cb37-2"></a> <span class="fl">0.333</span>  <span class="fl">0.333</span>  <span class="fl">0.167</span>  <span class="fl">0.167</span> </span></code></pre></div>
<p><br></p>
<ul>
<li>
<strong>string_dissimilarity_matrix</strong> : it returns a string-dissimilarity-matrix using either the <em>dice</em>, <em>levenshtein</em> or <em>cosine distance</em>. The input can be a <em>character string vector</em> only. In case that the method is dice then the dice-coefficient (similarity) is calculated between two strings for a specific number of character n-grams ( dice_n_gram ). The <em>dice</em> and <em>levenshtein</em> methods are applied to words, whereas the <em>cosine</em> distance to word-sentences.</li>
</ul>
<p><br></p>
<p>For illustration purposes I’ll use the previously mentioned <em>‘freq_vocab.txt’</em> file, but first I have to convert the text file to a character vector,</p>
<p><br></p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># first initialization of token_stats </span>

<span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">'freq_vocab.txt'</span><span class="op">)</span>

<span class="va">tmp_vec</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">path_2vector</span><span class="op">(</span><span class="op">)</span>      <span class="co"># convert to vector</span>


<span class="co"># second initialization to compute the dissimilarity matrix </span>

<span class="va">init_tok</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>x_vec <span class="op">=</span> <span class="va">tmp_vec</span><span class="op">)</span>

<span class="va">res</span> <span class="op">=</span> <span class="va">init_tok</span><span class="op">$</span><span class="fu">string_dissimilarity_matrix</span><span class="op">(</span>dice_n_gram <span class="op">=</span> <span class="fl">2</span>, method <span class="op">=</span> <span class="st">"dice"</span>, 
                                         
                                          split_separator <span class="op">=</span> <span class="st">" "</span>, dice_thresh <span class="op">=</span> <span class="fl">1.0</span>, 
                                         
                                          upper <span class="op">=</span> <span class="cn">TRUE</span>, diagonal <span class="op">=</span> <span class="cn">TRUE</span>, threads <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a>                the      term     planet is   ancient      with      ties to   history  ....</span>
<span id="cb39-2"><a href="#cb39-2"></a>the       <span class="fl">0.0000000</span> <span class="fl">0.6000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">0.6000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-3"><a href="#cb39-3"></a>term      <span class="fl">0.6000000</span> <span class="fl">0.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">0.6666667</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-4"><a href="#cb39-4"></a>planet    <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">0.00000000</span>  <span class="dv">1</span> <span class="fl">0.8181818</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-5"><a href="#cb39-5"></a>is        <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-6"><a href="#cb39-6"></a>ancient   <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">0.81818182</span>  <span class="dv">1</span> <span class="fl">0.0000000</span> <span class="fl">1.0000000</span> <span class="fl">0.7777778</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-7"><a href="#cb39-7"></a>with      <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span> <span class="fl">0.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-8"><a href="#cb39-8"></a>ties      <span class="fl">0.6000000</span> <span class="fl">0.6666667</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">0.7777778</span> <span class="fl">1.0000000</span> <span class="fl">0.0000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-9"><a href="#cb39-9"></a>to        <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-10"><a href="#cb39-10"></a>history   <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">1</span> <span class="fl">0.0000000</span>  ....</span>
<span id="cb39-11"><a href="#cb39-11"></a>astrology <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">0.8571429</span> <span class="fl">1.0000000</span> <span class="fl">1.0000000</span>  <span class="dv">1</span> <span class="fl">0.8571429</span>  ....</span>
<span id="cb39-12"><a href="#cb39-12"></a>science   <span class="fl">1.0000000</span> <span class="fl">1.0000000</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">0.3333333</span> <span class="fl">1.0000000</span> <span class="fl">0.7777778</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>  ....</span>
<span id="cb39-13"><a href="#cb39-13"></a>.</span>
<span id="cb39-14"><a href="#cb39-14"></a>.</span>
<span id="cb39-15"><a href="#cb39-15"></a>.</span></code></pre></div>
<p><br></p>
<p>here by adjusting (reducing ) the <em>dice_thresh</em> parameter we can force values close to 1.0 to become 1.0,</p>
<p><br></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a>                the term     planet is   ancient with ties to history astrology   science ....</span>
<span id="cb40-2"><a href="#cb40-2"></a>the       <span class="fl">0.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-3"><a href="#cb40-3"></a>term      <span class="fl">1.0000000</span>    <span class="dv">0</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-4"><a href="#cb40-4"></a>planet    <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">0.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-5"><a href="#cb40-5"></a>is        <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">0</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-6"><a href="#cb40-6"></a>ancient   <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">0.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">0.3333333</span> ....</span>
<span id="cb40-7"><a href="#cb40-7"></a>with      <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">0</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-8"><a href="#cb40-8"></a>ties      <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">0</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-9"><a href="#cb40-9"></a>to        <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">0</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">0</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-10"><a href="#cb40-10"></a>history   <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">0</span>         <span class="dv">1</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-11"><a href="#cb40-11"></a>astrology <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">1.0000000</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">0</span> <span class="fl">1.0000000</span> ....</span>
<span id="cb40-12"><a href="#cb40-12"></a>science   <span class="fl">1.0000000</span>    <span class="dv">1</span> <span class="fl">1.00000000</span>  <span class="dv">1</span> <span class="fl">0.3333333</span>    <span class="dv">1</span>    <span class="dv">1</span>  <span class="dv">1</span>       <span class="dv">1</span>         <span class="dv">1</span> <span class="fl">0.0000000</span> .... </span>
<span id="cb40-13"><a href="#cb40-13"></a>.</span>
<span id="cb40-14"><a href="#cb40-14"></a>.</span>
<span id="cb40-15"><a href="#cb40-15"></a>.</span></code></pre></div>
<p><br></p>
<ul>
<li>
<strong>look_up_table</strong> : The idea here is to split the input words to n-grams using a numeric value and then retrieve the words which have a similar character n-gram. <br> It returns a look-up-list where the list-names are the n-grams and the list-vectors are the words associated with those n-grams. The words for each n-gram can be retrieved using the <em>print_words_lookup_tbl</em> method. The input can be a character string vector only.</li>
</ul>
<p><br></p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># first initialization of token_stats </span>

<span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">'freq_vocab.txt'</span><span class="op">)</span>

<span class="va">tmp_vec</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">path_2vector</span><span class="op">(</span><span class="op">)</span>    <span class="co"># convert to vector</span>


<span class="co"># second initialization to compute the look-up-table</span>

<span class="va">init_lk</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>x_vec <span class="op">=</span> <span class="va">tmp_vec</span><span class="op">)</span>

<span class="va">is_vec</span> <span class="op">=</span> <span class="va">init_lk</span><span class="op">$</span><span class="fu">look_up_table</span><span class="op">(</span>n_grams <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># example output for the 'is_vec' vector</span></span>
<span id="cb42-2"><a href="#cb42-2"></a></span>
<span id="cb42-3"><a href="#cb42-3"></a>[<span class="dv">1</span>] <span class="st">""</span>    <span class="st">"ake"</span> <span class="st">"_an"</span> <span class="st">"anc"</span> <span class="st">"ane"</span> <span class="st">"_as"</span> <span class="st">"ast"</span> <span class="st">"cie"</span> <span class="st">"eli"</span> <span class="st">"enc"</span> <span class="st">"era"</span> <span class="st">"eve"</span> ..... </span>
<span id="cb42-4"><a href="#cb42-4"></a>[<span class="dv">29</span>] <span class="st">"net"</span> <span class="st">"ola"</span> <span class="st">"olo"</span> <span class="st">"_pl"</span> <span class="st">"pla"</span> <span class="st">"_re"</span> <span class="st">"rel"</span> <span class="st">"rol"</span> <span class="st">"_sc"</span> <span class="st">"sci"</span> <span class="st">"_se"</span> <span class="st">"see"</span> .....</span>
<span id="cb42-5"><a href="#cb42-5"></a>[<span class="dv">57</span>] <span class="st">"tro"</span> <span class="st">"ver"</span> <span class="st">"_we"</span> <span class="st">"wer"</span> <span class="st">"_wi"</span> <span class="st">"wit"</span> <span class="st">"yst"</span> <span class="st">"yth"</span></span></code></pre></div>
<p><br></p>
<p>then retrieve words with same n-grams,</p>
<p><br></p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">init_lk</span><span class="op">$</span><span class="fu">print_words_lookup_tbl</span><span class="op">(</span>n_gram <span class="op">=</span> <span class="st">"log"</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="st">"_astrology_"</span> <span class="st">"_mythology_"</span></span></code></pre></div>
<p><br></p>
<p>the underscores are necessary to distinguish the begin and end of each word when computing the n-grams.</p>
<p>More information about the <em>token_stats</em> class can be found in the package documentation.</p>
<p><br><br></p>
</div>
<div id="helper-functions-for-sparse_matrices" class="section level2">
<h2 class="hasAnchor">
<a href="#helper-functions-for-sparse_matrices" class="anchor"></a><em>helper functions for sparse_matrices</em>
</h2>
<p><br></p>
<p>The purpose of creating those functions is because I observed that they return faster in comparison to other R packages. The following code chunks explain each one of the functions,</p>
<p><br></p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------------------</span>
<span class="co"># conversion from dense to sparse</span>
<span class="co">#---------------------------------</span>

<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mlampros/textTinyR">textTinyR</a></span><span class="op">)</span></code></pre></div>
<pre><code>## Loading required package: Matrix</code></pre>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>
<span class="va">dsm</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, <span class="fl">100</span>, replace <span class="op">=</span> <span class="cn">T</span><span class="op">)</span>, <span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span>

<span class="va">res_sp</span> <span class="op">=</span> <span class="fu"><a href="../reference/dense_2sparse.html">dense_2sparse</a></span><span class="op">(</span><span class="va">dsm</span><span class="op">)</span>

<span class="va">res_sp</span></code></pre></div>
<pre><code>## 10 x 10 sparse Matrix of class "dgCMatrix"
##                          
##  [1,] . . . 1 1 1 . 1 1 .
##  [2,] 1 . . 1 1 . 1 1 1 .
##  [3,] . . . . . 1 1 . . 1
##  [4,] . . . 1 1 1 1 . . 1
##  [5,] 1 . . . 1 . 1 . . .
##  [6,] . 1 . . 1 . 1 1 1 .
##  [7,] . 1 1 1 1 1 1 1 . .
##  [8,] . 1 . . 1 1 . . 1 1
##  [9,] 1 1 . 1 . 1 1 . . 1
## [10,] 1 . 1 1 . . 1 1 1 .</code></pre>
<p><br></p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------</span>
<span class="co"># row- or column- sums</span>
<span class="co">#---------------------</span>

<span class="va">sm_cols</span> <span class="op">=</span> <span class="fu"><a href="../reference/sparse_Sums.html">sparse_Sums</a></span><span class="op">(</span><span class="va">res_sp</span>, rowSums <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>       <span class="co"># column-sums</span>

<span class="va">sm_cols</span></code></pre></div>
<pre><code>##  [1] 4 4 2 6 7 6 8 5 5 4</code></pre>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sm_rows</span> <span class="op">=</span> <span class="fu"><a href="../reference/sparse_Sums.html">sparse_Sums</a></span><span class="op">(</span><span class="va">res_sp</span>, rowSums <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>        <span class="co"># row-sums</span>

<span class="va">sm_rows</span></code></pre></div>
<pre><code>##  [1] 5 6 3 5 3 5 7 5 6 6</code></pre>
<p><br></p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#----------------------</span>
<span class="co"># row- or column- means</span>
<span class="co">#----------------------</span>

<span class="va">mn_cols</span> <span class="op">=</span> <span class="fu"><a href="../reference/sparse_Means.html">sparse_Means</a></span><span class="op">(</span><span class="va">res_sp</span>, rowMeans <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>    <span class="co"># column-means</span>

<span class="va">mn_cols</span></code></pre></div>
<pre><code>##  [1] 0.4 0.4 0.2 0.6 0.7 0.6 0.8 0.5 0.5 0.4</code></pre>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mn_rows</span> <span class="op">=</span> <span class="fu"><a href="../reference/sparse_Means.html">sparse_Means</a></span><span class="op">(</span><span class="va">res_sp</span>, rowMeans <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>    <span class="co"># row-means</span>

<span class="va">mn_rows</span></code></pre></div>
<pre><code>##  [1] 0.5 0.6 0.3 0.5 0.3 0.5 0.7 0.5 0.6 0.6</code></pre>
<p><br></p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#-------------------------------------</span>
<span class="co"># sparsity of a matrix (as percentage)</span>
<span class="co">#-------------------------------------</span>

<span class="fu"><a href="../reference/matrix_sparsity.html">matrix_sparsity</a></span><span class="op">(</span><span class="va">res_sp</span><span class="op">)</span></code></pre></div>
<pre><code>## 49 %</code></pre>
<p><br></p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#------------------------------------------------------</span>
<span class="co"># saving and loading sparse matrices (in binary format)</span>
<span class="co">#------------------------------------------------------</span>

<span class="va">save_sp</span> <span class="op">=</span> <span class="fu"><a href="../reference/save_sparse_binary.html">save_sparse_binary</a></span><span class="op">(</span><span class="va">res_sp</span>, file_name <span class="op">=</span> <span class="st">"save_sparse.mat"</span><span class="op">)</span>

<span class="va">load_sp</span> <span class="op">=</span> <span class="fu"><a href="../reference/load_sparse_binary.html">load_sparse_binary</a></span><span class="op">(</span>file_name <span class="op">=</span> <span class="st">"save_sparse.mat"</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>More information about the helper functions for sparse matrices can be found in the package documentation.</p>
<p><br><br></p>
</div>
<div id="tokenization" class="section level2">
<h2 class="hasAnchor">
<a href="#tokenization" class="anchor"></a><em>tokenization</em>
</h2>
<p><br></p>
<p>The <strong>tokenize_transform_text()</strong> function applies tokenization and transformation in a similar way to the <em>big_text_tokenizer()</em> method, however for small to medium data sets. The input can be either a character string (text data) or a path to a file. This method takes as input a single character string (character-string == one line). The parameters for the <em>tokenize_transform_text()</em> function are the same to the (already explained) <em>big_text_tokenizer()</em> method with the only exception being the input data type.</p>
<p><br></p>
<p>The <strong>tokenize_transform_vec_docs()</strong> function works in the same way to the <em>Term_Matrix()</em> method and it targets small to medium data sets. It takes as input a vector of documents and retains their order after tokenization and transformation has taken place. Both the <em>tokenize_transform_text()</em> and <em>tokenize_transform_vec_docs()</em> share the same parameters, with the following two exceptions,</p>
<ul>
<li>the <strong>object</strong> is a character vector</li>
<li>the <strong>as_token</strong> parameter : if TRUE then the output of the function is a list of (split) token. Otherwise it’s a vector of character strings (sentences)</li>
</ul>
<p><br></p>
<p>The following code chunks give an overview of the mentioned functions,</p>
<p><br></p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#------------------------</span>
<span class="co"># tokenize_transform_text</span>
<span class="co">#------------------------</span>


<span class="co"># example input : "planets.txt"</span>


<span class="va">res_txt</span> <span class="op">=</span> <span class="fu"><a href="../reference/tokenize_transform_text.html">tokenize_transform_text</a></span><span class="op">(</span>object <span class="op">=</span> <span class="st">"/planets.txt"</span>, 
                                  
                                  to_lower <span class="op">=</span> <span class="cn">TRUE</span>,
                                  
                                  utf_locale <span class="op">=</span> <span class="st">""</span>,           
                                  
                                  trim_token <span class="op">=</span> <span class="cn">TRUE</span>,
                                  
                                  split_string <span class="op">=</span> <span class="cn">TRUE</span>,
                                  
                                  remove_stopwords <span class="op">=</span> <span class="cn">TRUE</span>, 
                                  
                                  language <span class="op">=</span> <span class="st">"english"</span>,
                                  
                                  stemmer <span class="op">=</span> <span class="st">"ngram_sequential"</span>,
                                      
                                  stemmer_ngram <span class="op">=</span> <span class="fl">3</span>,
                                      
                                  threads <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>the output is a vector of <em>tokens</em> after the english stopwords were removed and the terms were stemmed (<em>ngram_sequential</em> of length 3),</p>
<p><br></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1"></a><span class="co"># example output :</span></span>
<span id="cb61-2"><a href="#cb61-2"></a></span>
<span id="cb61-3"><a href="#cb61-3"></a><span class="op">$</span>token</span>
<span id="cb61-4"><a href="#cb61-4"></a>  [<span class="dv">1</span>] <span class="st">"ter"</span>          <span class="st">"planet"</span>        <span class="st">"anci"</span>         <span class="st">"ties"</span>         <span class="st">"hist"</span>  ...</span>
<span id="cb61-5"><a href="#cb61-5"></a></span>
<span id="cb61-6"><a href="#cb61-6"></a>  [<span class="dv">16</span>] <span class="st">"early"</span>        <span class="st">"cultu"</span>        <span class="st">"divi"</span>         <span class="st">"emissar"</span>      <span class="st">"deit"</span> ....</span>
<span id="cb61-7"><a href="#cb61-7"></a></span>
<span id="cb61-8"><a href="#cb61-8"></a>  [<span class="dv">31</span>] <span class="st">"object"</span>       <span class="st">"2006"</span>         <span class="st">"internatio"</span>   <span class="st">"astro"</span>        <span class="st">"union"</span> ...  </span>
<span id="cb61-9"><a href="#cb61-9"></a></span>
<span id="cb61-10"><a href="#cb61-10"></a>  [<span class="dv">46</span>] <span class="st">"exclu"</span>        <span class="st">"object"</span>       <span class="st">"planet"</span>        <span class="st">"mass"</span>         <span class="st">"based"" ....</span></span>
<span id="cb61-11"><a href="#cb61-11"></a><span class="st">.</span></span>
<span id="cb61-12"><a href="#cb61-12"></a><span class="st">.</span></span>
<span id="cb61-13"><a href="#cb61-13"></a><span class="st">.</span></span>
<span id="cb61-14"><a href="#cb61-14"></a></span>
<span id="cb61-15"><a href="#cb61-15"></a><span class="st">attr(,"</span>class<span class="st">")</span></span>
<span id="cb61-16"><a href="#cb61-16"></a><span class="st">[1] "</span>tokenization and transformation<span class="st">"</span></span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#----------------------------</span>
<span class="co"># tokenize_transform_vec_docs</span>
<span class="co">#----------------------------</span>


<span class="co"># the input should be a vector of documents</span>

<span class="va">init</span> <span class="op">=</span> <span class="va"><a href="../reference/token_stats.html">token_stats</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>path_2file <span class="op">=</span> <span class="st">"/planets.txt"</span><span class="op">)</span>

<span class="va">inp</span> <span class="op">=</span> <span class="va">init</span><span class="op">$</span><span class="fu">path_2vector</span><span class="op">(</span><span class="op">)</span>   <span class="co"># convert text file to character vector</span>


<span class="co"># run the function using the input-vector</span>

<span class="va">res_dct</span> <span class="op">=</span> <span class="fu"><a href="../reference/tokenize_transform_vec_docs.html">tokenize_transform_vec_docs</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">inp</span>, 
                                      
                                      as_token <span class="op">=</span> <span class="cn">FALSE</span>,  <span class="co"># return character vector of documents</span>
                                      
                                      to_lower <span class="op">=</span> <span class="cn">TRUE</span>,
                                  
                                      utf_locale <span class="op">=</span> <span class="st">""</span>,           
                                      
                                      trim_token <span class="op">=</span> <span class="cn">TRUE</span>,
                                      
                                      split_string <span class="op">=</span> <span class="cn">TRUE</span>,
                                      
                                      remove_stopwords <span class="op">=</span> <span class="cn">TRUE</span>, 
                                      
                                      language <span class="op">=</span> <span class="st">"english"</span>,
                                      
                                      stemmer <span class="op">=</span> <span class="st">"porter2_stemmer"</span>, 
                                  
                                      threads <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p><br></p>
<p>the output is a vector of <em>transformed documents</em> after the english stopwords were removed and the terms were stemmed (<em>porter2-stemming</em>),</p>
<p><br></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1"></a><span class="op">$</span>token</span>
<span id="cb63-2"><a href="#cb63-2"></a>[<span class="dv">1</span>] <span class="st">"term planet ancient tie histori astrolog scienc mytholog religion planet solar ....."</span></span>
<span id="cb63-3"><a href="#cb63-3"></a></span>
<span id="cb63-4"><a href="#cb63-4"></a>[<span class="dv">2</span>] <span class="st">"planetari bodi discov 1950 remain planet modern definit celesti bodi cere palla ....."</span></span>
<span id="cb63-5"><a href="#cb63-5"></a></span>
<span id="cb63-6"><a href="#cb63-6"></a>[<span class="dv">3</span>] <span class="st">"planet thought ptolemi orbit earth defer epicycl motion idea planet orbit sun ....."</span></span>
<span id="cb63-7"><a href="#cb63-7"></a></span>
<span id="cb63-8"><a href="#cb63-8"></a>[<span class="dv">4</span>] <span class="st">"time care analysi pre-telescop observ data collect tycho brahe johann kepler  ....."</span></span>
<span id="cb63-9"><a href="#cb63-9"></a></span>
<span id="cb63-10"><a href="#cb63-10"></a>[<span class="dv">5</span>] <span class="st">"planet general divid main type larg lowdens giant planet smaller rocki terrestri ....."</span></span>
<span id="cb63-11"><a href="#cb63-11"></a></span>
<span id="cb63-12"><a href="#cb63-12"></a></span>
<span id="cb63-13"><a href="#cb63-13"></a><span class="kw">attr</span>(,<span class="st">"class"</span>)</span>
<span id="cb63-14"><a href="#cb63-14"></a>[<span class="dv">1</span>] <span class="st">"tokenization and transformation"</span></span></code></pre></div>
<p><br></p>
<p>The documents can be returned as a list of character vectors by specifying, <em>as_token</em> = TRUE,</p>
<p><br></p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># run the function using the input-vector</span>

<span class="va">res_dct_tok</span> <span class="op">=</span> <span class="fu"><a href="../reference/tokenize_transform_vec_docs.html">tokenize_transform_vec_docs</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">inp</span>, 
                                      
                                          as_token <span class="op">=</span> <span class="cn">TRUE</span>,
                                          
                                          to_lower <span class="op">=</span> <span class="cn">TRUE</span>,
                                      
                                          utf_locale <span class="op">=</span> <span class="st">""</span>,           
                                          
                                          trim_token <span class="op">=</span> <span class="cn">TRUE</span>,
                                          
                                          split_string <span class="op">=</span> <span class="cn">TRUE</span>,
                                          
                                          remove_stopwords <span class="op">=</span> <span class="cn">TRUE</span>, 
                                          
                                          language <span class="op">=</span> <span class="st">"english"</span>,
                                          
                                          stemmer <span class="op">=</span> <span class="st">"porter2_stemmer"</span>, 
                                  
                                          threads <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a><span class="op">$</span>token</span>
<span id="cb65-2"><a href="#cb65-2"></a><span class="op">$</span>token[[<span class="dv">1</span>]]</span>
<span id="cb65-3"><a href="#cb65-3"></a> [<span class="dv">1</span>] <span class="st">"term"</span>        <span class="st">"planet"</span>    <span class="st">"ancient"</span>  <span class="st">"tie"</span>   <span class="st">"histori"</span>   ..... </span>
<span id="cb65-4"><a href="#cb65-4"></a></span>
<span id="cb65-5"><a href="#cb65-5"></a><span class="op">$</span>token[[<span class="dv">2</span>]]</span>
<span id="cb65-6"><a href="#cb65-6"></a> [<span class="dv">1</span>] <span class="st">"planetari"</span>    <span class="st">"bodi"</span>     <span class="st">"discov"</span>   <span class="st">"1950"</span>   <span class="st">"remain"</span>   .....    </span>
<span id="cb65-7"><a href="#cb65-7"></a></span>
<span id="cb65-8"><a href="#cb65-8"></a><span class="op">$</span>token[[<span class="dv">3</span>]]</span>
<span id="cb65-9"><a href="#cb65-9"></a> [<span class="dv">1</span>] <span class="st">"planet"</span>       <span class="st">"thought"</span>  <span class="st">"ptolemi"</span>  <span class="st">"orbit"</span>   <span class="st">"earth"</span>   .....   </span>
<span id="cb65-10"><a href="#cb65-10"></a></span>
<span id="cb65-11"><a href="#cb65-11"></a></span>
<span id="cb65-12"><a href="#cb65-12"></a><span class="kw">attr</span>(,<span class="st">"class"</span>)</span>
<span id="cb65-13"><a href="#cb65-13"></a>[<span class="dv">1</span>] <span class="st">"tokenization and transformation"</span></span></code></pre></div>
<p><br></p>
<p>A few words about the <strong>utf_locale</strong>, <strong>remove_stopwords</strong> and <strong>stemmer</strong> parameters.</p>
<p><br></p>
<ul>
<li>The <strong>utf_locale</strong> can take as input either an empty string ("“) or a character string (for instance”el_GR.UTF-8"). It should be a non-empty string if the text input is other than english. However, currently for the windows OS only english character strings or files can be input and pre-processed.</li>
</ul>
<p><br></p>
<ul>
<li>The <strong>remove_stopwords</strong> parameter can be either a boolean (TRUE, FALSE) or a character vector of user defined stop-words. The available languages are specified by the parameter <strong>language</strong>. Currently, there is no support for chinese, japanese, korean, thai or languages with ambiguous word boundaries.</li>
</ul>
<p><br></p>
<ul>
<li>The <strong>stemmer</strong> parameter can take as input one of the <strong>porter2_stemmer</strong>, <strong>ngram_sequential</strong> or <strong>ngram_overlap</strong>.
<ul>
<li>The <a href="https://github.com/smassung/porter2_stemmer"><em>porter2_stemmer</em></a> is a C++ implementation of the <a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">snowball-porter2</a> stemming algorithm. The <em>porter2_stemmer</em> applies to all functions of the <em>textTinyR</em> package.</li>
<li>On the other hand, <em>n-gram stemming</em> is <em>“language independent”</em> and supported by the <strong>ngram_sequential</strong> and <strong>ngram_overlap</strong> functions. The <em>n-gram stemming</em> applies to all functions except for the <em>sparse_term_matrix</em> and <em>tokenize_transform_vec_docs</em> functions of the <em>textTinyR</em> package.
<ul>
<li>
<a href="http://clef.isti.cnr.it/2007/working_notes/mcnameeCLEF2007.pdf"><em>ngram_overlap</em></a> : The ngram_overlap stemming method is based on N-Gram Morphemes for Retrieval, Paul McNamee and James Mayfield</li>
<li>
<a href="https://arxiv.org/pdf/1312.4824.pdf"><em>ngram_sequential</em></a> : The ngram_sequential stemming method is a modified version based on Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm, B. P. Pande, Pawan Tamta, H. S. Dhami.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br><br></p>
</div>
<div id="utility-functions" class="section level2">
<h2 class="hasAnchor">
<a href="#utility-functions" class="anchor"></a><em>utility</em> functions</h2>
<p><br></p>
<p>The following code chunks illustrate the <em>utility functions</em> of the package (besides the <strong>read_characters()</strong> and <strong>read_rows()</strong> which used in the previous code chunks),</p>
<p><br></p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------------------------</span>
<span class="co"># cosine distance between word sentences</span>
<span class="co">#---------------------------------------</span>

<span class="va">s1</span> <span class="op">=</span> <span class="st">'sentence with two words'</span>

<span class="va">s2</span> <span class="op">=</span> <span class="st">'sentence with three words'</span>

<span class="va">sep</span> <span class="op">=</span> <span class="st">" "</span>

<span class="fu"><a href="../reference/cosine_distance.html">cosine_distance</a></span><span class="op">(</span><span class="va">s1</span>, <span class="va">s2</span>, split_separator <span class="op">=</span> <span class="va">sep</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.75</code></pre>
<p><br></p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#------------------------------------------------------------------------</span>
<span class="co"># dice distance between two words (using n-grams -- the lower the better)</span>
<span class="co">#------------------------------------------------------------------------</span>

<span class="va">w1</span> <span class="op">=</span> <span class="st">'word_one'</span>

<span class="va">w2</span> <span class="op">=</span> <span class="st">'word_two'</span>

<span class="va">n</span> <span class="op">=</span> <span class="fl">2</span>

<span class="fu"><a href="../reference/dice_distance.html">dice_distance</a></span><span class="op">(</span><span class="va">w1</span>, <span class="va">w2</span>, n_grams <span class="op">=</span> <span class="va">n</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 0.4117647</code></pre>
<p><br></p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------------------------</span>
<span class="co"># levenshtein distance between two words</span>
<span class="co">#---------------------------------------</span>

<span class="va">w1</span> <span class="op">=</span> <span class="st">'word_two'</span>

<span class="va">w2</span> <span class="op">=</span> <span class="st">'word_one'</span>

<span class="fu"><a href="../reference/levenshtein_distance.html">levenshtein_distance</a></span><span class="op">(</span><span class="va">w1</span>, <span class="va">w2</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p><br></p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------------------------------</span>
<span class="co"># bytes converter (returns the size of a file)</span>
<span class="co">#---------------------------------------------</span>

<span class="va">PATH</span> <span class="op">=</span> <span class="st">"/planets.txt"</span>

<span class="fu"><a href="../reference/bytes_converter.html">bytes_converter</a></span><span class="op">(</span>input_path_file <span class="op">=</span> <span class="va">PATH</span>, unit <span class="op">=</span> <span class="st">"KB"</span> <span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">## [1] 2.213867</span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#---------------------------------------------------</span>
<span class="co"># returns the utf-locale for the available languages</span>
<span class="co">#---------------------------------------------------</span>


<span class="fu"><a href="../reference/utf_locale.html">utf_locale</a></span><span class="op">(</span>language <span class="op">=</span> <span class="st">"english"</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "en.UTF-8"</code></pre>
<p><br></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1"></a><span class="co">#-----------------</span></span>
<span id="cb76-2"><a href="#cb76-2"></a><span class="co"># text file parser</span></span>
<span id="cb76-3"><a href="#cb76-3"></a><span class="co">#-----------------</span></span>
<span id="cb76-4"><a href="#cb76-4"></a></span>
<span id="cb76-5"><a href="#cb76-5"></a><span class="co"># The text file should have a structure (such as an xml-structure), so that </span></span>
<span id="cb76-6"><a href="#cb76-6"></a><span class="co"># subsets can be extracted using the "start_query" and "end_query" parameters.</span></span>
<span id="cb76-7"><a href="#cb76-7"></a><span class="co"># (it works similarly to the big_text_parser() method, however for small to medium sized files)</span></span>
<span id="cb76-8"><a href="#cb76-8"></a></span>
<span id="cb76-9"><a href="#cb76-9"></a><span class="co"># example input "example_file.xml" file :</span></span>
<span id="cb76-10"><a href="#cb76-10"></a></span>
<span id="cb76-11"><a href="#cb76-11"></a><span class="op">&lt;</span>?xml version=<span class="st">"1.0"</span>?<span class="op">&gt;</span></span>
<span id="cb76-12"><a href="#cb76-12"></a><span class="er">&lt;</span>sked<span class="op">&gt;</span></span>
<span id="cb76-13"><a href="#cb76-13"></a><span class="st">  </span><span class="er">&lt;</span>version<span class="op">&gt;</span><span class="dv">2</span><span class="op">&lt;</span><span class="er">/</span>version<span class="op">&gt;</span></span>
<span id="cb76-14"><a href="#cb76-14"></a><span class="st">  </span><span class="er">&lt;</span>flight<span class="op">&gt;</span></span>
<span id="cb76-15"><a href="#cb76-15"></a><span class="st">    </span><span class="er">&lt;</span>carrier<span class="op">&gt;</span>BA<span class="op">&lt;</span><span class="er">/</span>carrier<span class="op">&gt;</span></span>
<span id="cb76-16"><a href="#cb76-16"></a><span class="st">    </span><span class="er">&lt;</span>number<span class="op">&gt;</span><span class="dv">4001</span><span class="op">&lt;</span><span class="er">/</span>number<span class="op">&gt;</span></span>
<span id="cb76-17"><a href="#cb76-17"></a><span class="st">    </span><span class="er">&lt;</span>date<span class="op">&gt;</span><span class="dv">2011-07-21</span><span class="op">&lt;</span><span class="er">/</span>date<span class="op">&gt;</span></span>
<span id="cb76-18"><a href="#cb76-18"></a><span class="st">  </span><span class="er">&lt;/</span>flight<span class="op">&gt;</span></span>
<span id="cb76-19"><a href="#cb76-19"></a><span class="st">  </span><span class="er">&lt;</span>flight cancelled=<span class="st">"true"</span><span class="op">&gt;</span></span>
<span id="cb76-20"><a href="#cb76-20"></a><span class="st">    </span><span class="er">&lt;</span>carrier<span class="op">&gt;</span>BA<span class="op">&lt;</span><span class="er">/</span>carrier<span class="op">&gt;</span></span>
<span id="cb76-21"><a href="#cb76-21"></a><span class="st">    </span><span class="er">&lt;</span>number<span class="op">&gt;</span><span class="dv">4002</span><span class="op">&lt;</span><span class="er">/</span>number<span class="op">&gt;</span></span>
<span id="cb76-22"><a href="#cb76-22"></a><span class="st">    </span><span class="er">&lt;</span>date<span class="op">&gt;</span><span class="dv">2011-07-21</span><span class="op">&lt;</span><span class="er">/</span>date<span class="op">&gt;</span></span>
<span id="cb76-23"><a href="#cb76-23"></a><span class="st">  </span><span class="er">&lt;/</span>flight<span class="op">&gt;</span></span>
<span id="cb76-24"><a href="#cb76-24"></a><span class="er">&lt;/</span>sked<span class="op">&gt;</span></span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">fp</span> <span class="op">=</span> <span class="fu"><a href="../reference/text_file_parser.html">text_file_parser</a></span><span class="op">(</span>input_path_file <span class="op">=</span> <span class="st">"example_file.xml"</span>, 
                      
                      output_path_file <span class="op">=</span> <span class="st">"/output_folder/example_output_file.txt"</span>, 
                      
                      start_query <span class="op">=</span> <span class="st">'&lt;number&gt;'</span>, end_query <span class="op">=</span> <span class="st">'&lt;/number&gt;'</span>,
                      
                      min_lines <span class="op">=</span> <span class="fl">1</span>, trimmed_line <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="st">"example_output_file.txt"</span> <span class="op">:</span>
  
<span class="fl">4001</span>
<span class="fl">4002</span></code></pre></div>
<p><br></p>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#------------------</span>
<span class="co"># vocabulary parser</span>
<span class="co">#------------------</span>

<span class="co"># the 'vocabulary_parser' function extracts a vocabulary from a structured text (such as </span>
<span class="co"># an .xml file) and works in the exact same way as the 'big_tokenize_transform' class, </span>
<span class="co"># however for small to medium sized data files</span>


<span class="va">pars_dat</span> <span class="op">=</span> <span class="fu"><a href="../reference/vocabulary_parser.html">vocabulary_parser</a></span><span class="op">(</span>input_path_file <span class="op">=</span> <span class="st">'/folder/input_data.txt'</span>,
                             
                             start_query <span class="op">=</span> <span class="st">'start_word'</span>, end_query <span class="op">=</span> <span class="st">'end_word'</span>,
                             
                             vocabulary_path_file <span class="op">=</span> <span class="st">'/folder/vocab.txt'</span>, 
                             
                             to_lower <span class="op">=</span> <span class="cn">TRUE</span>, split_string <span class="op">=</span> <span class="cn">TRUE</span>,
                             
                             remove_stopwords <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Lampros Mouselimis.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
